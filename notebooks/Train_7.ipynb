{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b71cdd8a-fc30-46d2-803c-b7ee6eb49cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "991aa36b-ed88-41b8-885c-808b77f4962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"file:///media/sdb1/mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "424d6e82-8c48-4970-8a97-703b6385b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from models.models import build_model     \n",
    "from datasets.loader import DataModule, DataConfig  \n",
    "from train.trainer import Trainer, TrainConfig  \n",
    "from train.eval import Evaluator             \n",
    "from datasets.base import collate_bb\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_CLASSES = 1 + 24\n",
    "model_uri = \"file:///media/sdb1/mlflow/753485487056022103/a7b0ebbbad47442c841112c6bfb35e16/artifacts/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "\n",
    "def allow_missing_masks(model):\n",
    "    orig_forward = model.forward\n",
    "    def forward(images, targets=None):\n",
    "        if model.training and targets is not None and not all((\"masks\" in t) for t in targets):\n",
    "            rh = model.roi_heads\n",
    "            saved = (rh.mask_roi_pool, rh.mask_head, rh.mask_predictor)\n",
    "            try:\n",
    "                rh.mask_roi_pool, rh.mask_head, rh.mask_predictor = None, None, None\n",
    "                return orig_forward(images, targets)\n",
    "            finally:\n",
    "                rh.mask_roi_pool, rh.mask_head, rh.mask_predictor = saved\n",
    "        return orig_forward(images, targets)\n",
    "    model.forward = forward\n",
    "    return model\n",
    "model = allow_missing_masks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5203de-16bc-4bd5-8917-4f69b533ae68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 001/010] step 50/2250 loss 0.3688\n",
      "[epoch 001/010] step 100/2250 loss 0.3297\n",
      "[epoch 001/010] step 150/2250 loss 0.3659\n",
      "[epoch 001/010] step 200/2250 loss 0.3141\n",
      "[epoch 001/010] step 250/2250 loss 0.3029\n",
      "[epoch 001/010] step 300/2250 loss 0.3593\n",
      "[epoch 001/010] step 350/2250 loss 0.4053\n",
      "[epoch 001/010] step 400/2250 loss 0.3832\n",
      "[epoch 001/010] step 450/2250 loss 0.3498\n",
      "[epoch 001/010] step 500/2250 loss 0.3179\n",
      "[epoch 001/010] step 550/2250 loss 0.4044\n",
      "[epoch 001/010] step 600/2250 loss 0.3874\n",
      "[epoch 001/010] step 650/2250 loss 0.2800\n",
      "[epoch 001/010] step 700/2250 loss 0.4401\n",
      "[epoch 001/010] step 750/2250 loss 0.2957\n",
      "[epoch 001/010] step 800/2250 loss 0.3686\n",
      "[epoch 001/010] step 850/2250 loss 0.3180\n",
      "[epoch 001/010] step 900/2250 loss 0.3338\n",
      "[epoch 001/010] step 950/2250 loss 0.3585\n",
      "[epoch 001/010] step 1000/2250 loss 0.2928\n",
      "[epoch 001/010] step 1050/2250 loss 0.3490\n",
      "[epoch 001/010] step 1100/2250 loss 0.3315\n",
      "[epoch 001/010] step 1150/2250 loss 0.3641\n",
      "[epoch 001/010] step 1200/2250 loss 0.3128\n",
      "[epoch 001/010] step 1250/2250 loss 0.3993\n",
      "[epoch 001/010] step 1300/2250 loss 0.4457\n",
      "[epoch 001/010] step 1350/2250 loss 0.3470\n",
      "[epoch 001/010] step 1400/2250 loss 0.3177\n",
      "[epoch 001/010] step 1450/2250 loss 0.3454\n",
      "[epoch 001/010] step 1500/2250 loss 0.3350\n",
      "[epoch 001/010] step 1550/2250 loss 0.3875\n",
      "[epoch 001/010] step 1600/2250 loss 0.3603\n",
      "[epoch 001/010] step 1650/2250 loss 0.3147\n",
      "[epoch 001/010] step 1700/2250 loss 0.4243\n",
      "[epoch 001/010] step 1750/2250 loss 0.3474\n",
      "[epoch 001/010] step 1800/2250 loss 0.3998\n",
      "[epoch 001/010] step 1850/2250 loss 0.3628\n",
      "[epoch 001/010] step 1900/2250 loss 0.3281\n",
      "[epoch 001/010] step 1950/2250 loss 0.3573\n",
      "[epoch 001/010] step 2000/2250 loss 0.3601\n",
      "[epoch 001/010] step 2050/2250 loss 0.3371\n",
      "[epoch 001/010] step 2100/2250 loss 0.3691\n",
      "[epoch 001/010] step 2150/2250 loss 0.3771\n",
      "[epoch 001/010] step 2200/2250 loss 0.3731\n",
      "[epoch 001/010] step 2250/2250 loss 0.4005\n",
      "epoch 001/010  train=0.3950  val=0.5248\n",
      "[epoch 002/010] step 50/2250 loss 0.3961\n",
      "[epoch 002/010] step 100/2250 loss 0.2542\n",
      "[epoch 002/010] step 150/2250 loss 0.3451\n",
      "[epoch 002/010] step 200/2250 loss 0.3934\n",
      "[epoch 002/010] step 250/2250 loss 0.3687\n",
      "[epoch 002/010] step 300/2250 loss 0.2384\n",
      "[epoch 002/010] step 350/2250 loss 0.3756\n",
      "[epoch 002/010] step 400/2250 loss 0.3610\n",
      "[epoch 002/010] step 450/2250 loss 0.2833\n",
      "[epoch 002/010] step 500/2250 loss 0.3371\n",
      "[epoch 002/010] step 550/2250 loss 0.3462\n",
      "[epoch 002/010] step 600/2250 loss 0.3968\n",
      "[epoch 002/010] step 650/2250 loss 0.4091\n",
      "[epoch 002/010] step 700/2250 loss 0.3431\n",
      "[epoch 002/010] step 750/2250 loss 0.3472\n",
      "[epoch 002/010] step 800/2250 loss 0.3416\n",
      "[epoch 002/010] step 850/2250 loss 0.3160\n",
      "[epoch 002/010] step 900/2250 loss 0.3550\n",
      "[epoch 002/010] step 950/2250 loss 0.3471\n",
      "[epoch 002/010] step 1000/2250 loss 0.3296\n",
      "[epoch 002/010] step 1050/2250 loss 0.3023\n",
      "[epoch 002/010] step 1100/2250 loss 0.4387\n",
      "[epoch 002/010] step 1150/2250 loss 0.4779\n",
      "[epoch 002/010] step 1200/2250 loss 0.3775\n",
      "[epoch 002/010] step 1250/2250 loss 0.4064\n",
      "[epoch 002/010] step 1300/2250 loss 0.3405\n",
      "[epoch 002/010] step 1350/2250 loss 0.3885\n",
      "[epoch 002/010] step 1400/2250 loss 0.3727\n",
      "[epoch 002/010] step 1450/2250 loss 0.3001\n",
      "[epoch 002/010] step 1500/2250 loss 0.3648\n",
      "[epoch 002/010] step 1550/2250 loss 0.2930\n",
      "[epoch 002/010] step 1600/2250 loss 0.3253\n",
      "[epoch 002/010] step 1650/2250 loss 0.3388\n",
      "[epoch 002/010] step 1700/2250 loss 0.4571\n",
      "[epoch 002/010] step 1750/2250 loss 0.3463\n",
      "[epoch 002/010] step 1800/2250 loss 0.4101\n",
      "[epoch 002/010] step 1850/2250 loss 0.3094\n",
      "[epoch 002/010] step 1900/2250 loss 0.2626\n",
      "[epoch 002/010] step 1950/2250 loss 0.2706\n",
      "[epoch 002/010] step 2000/2250 loss 0.3874\n",
      "[epoch 002/010] step 2050/2250 loss 0.3192\n",
      "[epoch 002/010] step 2100/2250 loss 0.4246\n",
      "[epoch 002/010] step 2150/2250 loss 0.3412\n",
      "[epoch 002/010] step 2200/2250 loss 0.3937\n",
      "[epoch 002/010] step 2250/2250 loss 0.3688\n",
      "epoch 002/010  train=0.3895  val=0.5097\n",
      "[epoch 003/010] step 50/2250 loss 0.3959\n",
      "[epoch 003/010] step 100/2250 loss 0.3298\n",
      "[epoch 003/010] step 150/2250 loss 0.3615\n",
      "[epoch 003/010] step 200/2250 loss 0.3869\n",
      "[epoch 003/010] step 250/2250 loss 0.4217\n",
      "[epoch 003/010] step 300/2250 loss 0.2805\n",
      "[epoch 003/010] step 350/2250 loss 0.2732\n",
      "[epoch 003/010] step 400/2250 loss 0.3592\n",
      "[epoch 003/010] step 450/2250 loss 0.3421\n",
      "[epoch 003/010] step 500/2250 loss 0.4549\n",
      "[epoch 003/010] step 550/2250 loss 0.2944\n",
      "[epoch 003/010] step 600/2250 loss 0.3689\n",
      "[epoch 003/010] step 650/2250 loss 0.3680\n",
      "[epoch 003/010] step 700/2250 loss 0.2917\n",
      "[epoch 003/010] step 750/2250 loss 0.3455\n",
      "[epoch 003/010] step 800/2250 loss 0.3095\n",
      "[epoch 003/010] step 850/2250 loss 0.3567\n",
      "[epoch 003/010] step 900/2250 loss 0.3402\n",
      "[epoch 003/010] step 950/2250 loss 0.3175\n",
      "[epoch 003/010] step 1000/2250 loss 0.4227\n",
      "[epoch 003/010] step 1050/2250 loss 0.2493\n",
      "[epoch 003/010] step 1100/2250 loss 0.3415\n",
      "[epoch 003/010] step 1150/2250 loss 0.3826\n",
      "[epoch 003/010] step 1200/2250 loss 0.3162\n",
      "[epoch 003/010] step 1250/2250 loss 0.3393\n",
      "[epoch 003/010] step 1300/2250 loss 0.3375\n",
      "[epoch 003/010] step 1350/2250 loss 0.3182\n",
      "[epoch 003/010] step 1400/2250 loss 0.3879\n",
      "[epoch 003/010] step 1450/2250 loss 0.3512\n",
      "[epoch 003/010] step 1500/2250 loss 0.3410\n",
      "[epoch 003/010] step 1550/2250 loss 0.4179\n",
      "[epoch 003/010] step 1600/2250 loss 0.3561\n",
      "[epoch 003/010] step 1650/2250 loss 0.3469\n",
      "[epoch 003/010] step 1700/2250 loss 0.3874\n",
      "[epoch 003/010] step 1750/2250 loss 0.4130\n",
      "[epoch 003/010] step 1800/2250 loss 0.3187\n",
      "[epoch 003/010] step 1850/2250 loss 0.3549\n",
      "[epoch 003/010] step 1900/2250 loss 0.3268\n",
      "[epoch 003/010] step 1950/2250 loss 0.3376\n",
      "[epoch 003/010] step 2000/2250 loss 0.3891\n",
      "[epoch 003/010] step 2050/2250 loss 0.3585\n",
      "[epoch 003/010] step 2100/2250 loss 0.3720\n",
      "[epoch 003/010] step 2150/2250 loss 0.4120\n",
      "[epoch 003/010] step 2200/2250 loss 0.3646\n",
      "[epoch 003/010] step 2250/2250 loss 0.3916\n",
      "epoch 003/010  train=0.3859  val=0.5272\n",
      "[epoch 004/010] step 50/2250 loss 0.3396\n",
      "[epoch 004/010] step 100/2250 loss 0.3322\n",
      "[epoch 004/010] step 150/2250 loss 0.2973\n",
      "[epoch 004/010] step 200/2250 loss 0.3422\n",
      "[epoch 004/010] step 250/2250 loss 0.3451\n",
      "[epoch 004/010] step 300/2250 loss 0.3219\n",
      "[epoch 004/010] step 350/2250 loss 0.3650\n",
      "[epoch 004/010] step 400/2250 loss 0.3279\n",
      "[epoch 004/010] step 450/2250 loss 0.3915\n",
      "[epoch 004/010] step 500/2250 loss 0.3500\n",
      "[epoch 004/010] step 550/2250 loss 0.3019\n",
      "[epoch 004/010] step 600/2250 loss 0.3237\n",
      "[epoch 004/010] step 650/2250 loss 0.3608\n",
      "[epoch 004/010] step 700/2250 loss 0.3829\n",
      "[epoch 004/010] step 750/2250 loss 0.3476\n",
      "[epoch 004/010] step 800/2250 loss 0.3486\n",
      "[epoch 004/010] step 850/2250 loss 0.2616\n",
      "[epoch 004/010] step 900/2250 loss 0.3098\n",
      "[epoch 004/010] step 950/2250 loss 0.3575\n",
      "[epoch 004/010] step 1000/2250 loss 0.3170\n",
      "[epoch 004/010] step 1050/2250 loss 0.2882\n",
      "[epoch 004/010] step 1100/2250 loss 0.2903\n",
      "[epoch 004/010] step 1150/2250 loss 0.2738\n",
      "[epoch 004/010] step 1200/2250 loss 0.3381\n",
      "[epoch 004/010] step 1250/2250 loss 0.4112\n",
      "[epoch 004/010] step 1300/2250 loss 0.2665\n",
      "[epoch 004/010] step 1350/2250 loss 0.3839\n",
      "[epoch 004/010] step 1400/2250 loss 0.3150\n",
      "[epoch 004/010] step 1450/2250 loss 0.3874\n",
      "[epoch 004/010] step 1500/2250 loss 0.3250\n",
      "[epoch 004/010] step 1550/2250 loss 0.3789\n",
      "[epoch 004/010] step 1600/2250 loss 0.3079\n",
      "[epoch 004/010] step 1650/2250 loss 0.3372\n",
      "[epoch 004/010] step 1700/2250 loss 0.4289\n",
      "[epoch 004/010] step 1750/2250 loss 0.3582\n",
      "[epoch 004/010] step 1800/2250 loss 0.3586\n",
      "[epoch 004/010] step 1850/2250 loss 0.4144\n",
      "[epoch 004/010] step 1900/2250 loss 0.3458\n",
      "[epoch 004/010] step 1950/2250 loss 0.3550\n",
      "[epoch 004/010] step 2000/2250 loss 0.3070\n",
      "[epoch 004/010] step 2050/2250 loss 0.3090\n",
      "[epoch 004/010] step 2100/2250 loss 0.3388\n",
      "[epoch 004/010] step 2150/2250 loss 0.3656\n",
      "[epoch 004/010] step 2200/2250 loss 0.4348\n",
      "[epoch 004/010] step 2250/2250 loss 0.3257\n",
      "epoch 004/010  train=0.3817  val=0.5158\n",
      "[epoch 005/010] step 50/2250 loss 0.3456\n",
      "[epoch 005/010] step 100/2250 loss 0.3351\n",
      "[epoch 005/010] step 150/2250 loss 0.3263\n",
      "[epoch 005/010] step 200/2250 loss 0.3511\n",
      "[epoch 005/010] step 250/2250 loss 0.3162\n",
      "[epoch 005/010] step 300/2250 loss 0.3229\n",
      "[epoch 005/010] step 350/2250 loss 0.2826\n",
      "[epoch 005/010] step 400/2250 loss 0.3453\n",
      "[epoch 005/010] step 450/2250 loss 0.2839\n",
      "[epoch 005/010] step 500/2250 loss 0.4079\n",
      "[epoch 005/010] step 550/2250 loss 0.2784\n",
      "[epoch 005/010] step 600/2250 loss 0.3316\n",
      "[epoch 005/010] step 650/2250 loss 0.3226\n",
      "[epoch 005/010] step 700/2250 loss 0.2929\n",
      "[epoch 005/010] step 750/2250 loss 0.3237\n",
      "[epoch 005/010] step 800/2250 loss 0.2965\n",
      "[epoch 005/010] step 850/2250 loss 0.3359\n",
      "[epoch 005/010] step 900/2250 loss 0.3255\n",
      "[epoch 005/010] step 950/2250 loss 0.2926\n",
      "[epoch 005/010] step 1000/2250 loss 0.3405\n",
      "[epoch 005/010] step 1050/2250 loss 0.3044\n",
      "[epoch 005/010] step 1100/2250 loss 0.3477\n",
      "[epoch 005/010] step 1150/2250 loss 0.3454\n",
      "[epoch 005/010] step 1200/2250 loss 0.3001\n",
      "[epoch 005/010] step 1250/2250 loss 0.3620\n",
      "[epoch 005/010] step 1300/2250 loss 0.3098\n",
      "[epoch 005/010] step 1350/2250 loss 0.3276\n",
      "[epoch 005/010] step 1400/2250 loss 0.2902\n",
      "[epoch 005/010] step 1450/2250 loss 0.3013\n",
      "[epoch 005/010] step 1500/2250 loss 0.3039\n",
      "[epoch 005/010] step 1550/2250 loss 0.3323\n",
      "[epoch 005/010] step 1600/2250 loss 0.3177\n",
      "[epoch 005/010] step 1650/2250 loss 0.3743\n",
      "[epoch 005/010] step 1700/2250 loss 0.4193\n",
      "[epoch 005/010] step 1750/2250 loss 0.3335\n",
      "[epoch 005/010] step 1800/2250 loss 0.2956\n",
      "[epoch 005/010] step 1850/2250 loss 0.4543\n",
      "[epoch 005/010] step 1900/2250 loss 0.3325\n",
      "[epoch 005/010] step 1950/2250 loss 0.3082\n",
      "[epoch 005/010] step 2000/2250 loss 0.3754\n",
      "[epoch 005/010] step 2050/2250 loss 0.2843\n",
      "[epoch 005/010] step 2100/2250 loss 0.3748\n",
      "[epoch 005/010] step 2150/2250 loss 0.3542\n",
      "[epoch 005/010] step 2200/2250 loss 0.3392\n",
      "[epoch 005/010] step 2250/2250 loss 0.2887\n",
      "epoch 005/010  train=0.3772  val=0.5376\n",
      "[epoch 006/010] step 50/2250 loss 0.3526\n",
      "[epoch 006/010] step 100/2250 loss 0.3314\n",
      "[epoch 006/010] step 150/2250 loss 0.3787\n",
      "[epoch 006/010] step 200/2250 loss 0.3654\n",
      "[epoch 006/010] step 250/2250 loss 0.3073\n",
      "[epoch 006/010] step 300/2250 loss 0.3031\n",
      "[epoch 006/010] step 350/2250 loss 0.3595\n",
      "[epoch 006/010] step 400/2250 loss 0.4390\n",
      "[epoch 006/010] step 450/2250 loss 0.3979\n",
      "[epoch 006/010] step 500/2250 loss 0.3991\n",
      "[epoch 006/010] step 550/2250 loss 0.3655\n",
      "[epoch 006/010] step 600/2250 loss 0.3335\n",
      "[epoch 006/010] step 650/2250 loss 0.3375\n",
      "[epoch 006/010] step 700/2250 loss 0.3657\n",
      "[epoch 006/010] step 750/2250 loss 0.3019\n",
      "[epoch 006/010] step 800/2250 loss 0.3515\n",
      "[epoch 006/010] step 850/2250 loss 0.3486\n",
      "[epoch 006/010] step 900/2250 loss 0.3217\n",
      "[epoch 006/010] step 950/2250 loss 0.3495\n",
      "[epoch 006/010] step 1000/2250 loss 0.2915\n",
      "[epoch 006/010] step 1050/2250 loss 0.4111\n",
      "[epoch 006/010] step 1100/2250 loss 0.2768\n",
      "[epoch 006/010] step 1150/2250 loss 0.3746\n",
      "[epoch 006/010] step 1200/2250 loss 0.3448\n",
      "[epoch 006/010] step 1250/2250 loss 0.3046\n",
      "[epoch 006/010] step 1300/2250 loss 0.3580\n",
      "[epoch 006/010] step 1350/2250 loss 0.2937\n",
      "[epoch 006/010] step 1400/2250 loss 0.3376\n",
      "[epoch 006/010] step 1450/2250 loss 0.3533\n",
      "[epoch 006/010] step 1500/2250 loss 0.3228\n",
      "[epoch 006/010] step 1550/2250 loss 0.3600\n",
      "[epoch 006/010] step 1600/2250 loss 0.2849\n",
      "[epoch 006/010] step 1650/2250 loss 0.3063\n",
      "[epoch 006/010] step 1700/2250 loss 0.3561\n",
      "[epoch 006/010] step 1750/2250 loss 0.3807\n",
      "[epoch 006/010] step 1800/2250 loss 0.3104\n",
      "[epoch 006/010] step 1850/2250 loss 0.3825\n",
      "[epoch 006/010] step 1900/2250 loss 0.3088\n",
      "[epoch 006/010] step 1950/2250 loss 0.3681\n",
      "[epoch 006/010] step 2000/2250 loss 0.2747\n",
      "[epoch 006/010] step 2050/2250 loss 0.3220\n",
      "[epoch 006/010] step 2100/2250 loss 0.3228\n",
      "[epoch 006/010] step 2150/2250 loss 0.3215\n",
      "[epoch 006/010] step 2200/2250 loss 0.3490\n",
      "[epoch 006/010] step 2250/2250 loss 0.2898\n",
      "epoch 006/010  train=0.3732  val=0.5210\n",
      "[epoch 007/010] step 50/2250 loss 0.3411\n",
      "[epoch 007/010] step 100/2250 loss 0.3713\n",
      "[epoch 007/010] step 150/2250 loss 0.3232\n",
      "[epoch 007/010] step 200/2250 loss 0.2974\n",
      "[epoch 007/010] step 250/2250 loss 0.2829\n",
      "[epoch 007/010] step 300/2250 loss 0.3279\n",
      "[epoch 007/010] step 350/2250 loss 0.2874\n",
      "[epoch 007/010] step 400/2250 loss 0.2966\n",
      "[epoch 007/010] step 450/2250 loss 0.4226\n",
      "[epoch 007/010] step 500/2250 loss 0.3892\n",
      "[epoch 007/010] step 550/2250 loss 0.3013\n",
      "[epoch 007/010] step 600/2250 loss 0.3585\n",
      "[epoch 007/010] step 650/2250 loss 0.3848\n",
      "[epoch 007/010] step 700/2250 loss 0.2868\n",
      "[epoch 007/010] step 750/2250 loss 0.2660\n",
      "[epoch 007/010] step 800/2250 loss 0.3565\n",
      "[epoch 007/010] step 850/2250 loss 0.2731\n",
      "[epoch 007/010] step 900/2250 loss 0.3511\n",
      "[epoch 007/010] step 950/2250 loss 0.3315\n",
      "[epoch 007/010] step 1000/2250 loss 0.3369\n",
      "[epoch 007/010] step 1050/2250 loss 0.3005\n",
      "[epoch 007/010] step 1100/2250 loss 0.3091\n",
      "[epoch 007/010] step 1150/2250 loss 0.3505\n",
      "[epoch 007/010] step 1200/2250 loss 0.3472\n",
      "[epoch 007/010] step 1250/2250 loss 0.2982\n",
      "[epoch 007/010] step 1300/2250 loss 0.2680\n",
      "[epoch 007/010] step 1350/2250 loss 0.3442\n",
      "[epoch 007/010] step 1400/2250 loss 0.3511\n",
      "[epoch 007/010] step 1450/2250 loss 0.3680\n",
      "[epoch 007/010] step 1500/2250 loss 0.3632\n",
      "[epoch 007/010] step 1550/2250 loss 0.3301\n",
      "[epoch 007/010] step 1600/2250 loss 0.3118\n",
      "[epoch 007/010] step 1650/2250 loss 0.3497\n",
      "[epoch 007/010] step 1700/2250 loss 0.3898\n",
      "[epoch 007/010] step 1750/2250 loss 0.3026\n",
      "[epoch 007/010] step 1800/2250 loss 0.3330\n",
      "[epoch 007/010] step 1850/2250 loss 0.3695\n",
      "[epoch 007/010] step 1900/2250 loss 0.3119\n",
      "[epoch 007/010] step 1950/2250 loss 0.3496\n",
      "[epoch 007/010] step 2000/2250 loss 0.2799\n",
      "[epoch 007/010] step 2050/2250 loss 0.2890\n",
      "[epoch 007/010] step 2100/2250 loss 0.3091\n",
      "[epoch 007/010] step 2150/2250 loss 0.3993\n",
      "[epoch 007/010] step 2200/2250 loss 0.3506\n",
      "[epoch 007/010] step 2250/2250 loss 0.2956\n",
      "epoch 007/010  train=0.3697  val=0.5192\n",
      "[epoch 008/010] step 50/2250 loss 0.3720\n",
      "[epoch 008/010] step 100/2250 loss 0.3602\n",
      "[epoch 008/010] step 150/2250 loss 0.2747\n",
      "[epoch 008/010] step 200/2250 loss 0.3719\n",
      "[epoch 008/010] step 250/2250 loss 0.3120\n",
      "[epoch 008/010] step 300/2250 loss 0.3985\n",
      "[epoch 008/010] step 350/2250 loss 0.3614\n",
      "[epoch 008/010] step 400/2250 loss 0.3249\n",
      "[epoch 008/010] step 450/2250 loss 0.3294\n",
      "[epoch 008/010] step 500/2250 loss 0.3697\n",
      "[epoch 008/010] step 550/2250 loss 0.4354\n",
      "[epoch 008/010] step 600/2250 loss 0.3332\n",
      "[epoch 008/010] step 650/2250 loss 0.4344\n",
      "[epoch 008/010] step 700/2250 loss 0.3480\n",
      "[epoch 008/010] step 750/2250 loss 0.3067\n",
      "[epoch 008/010] step 800/2250 loss 0.3123\n",
      "[epoch 008/010] step 850/2250 loss 0.3326\n",
      "[epoch 008/010] step 900/2250 loss 0.3188\n",
      "[epoch 008/010] step 950/2250 loss 0.2918\n",
      "[epoch 008/010] step 1000/2250 loss 0.2634\n",
      "[epoch 008/010] step 1050/2250 loss 0.3320\n",
      "[epoch 008/010] step 1100/2250 loss 0.2775\n",
      "[epoch 008/010] step 1150/2250 loss 0.3738\n",
      "[epoch 008/010] step 1200/2250 loss 0.2675\n",
      "[epoch 008/010] step 1250/2250 loss 0.2924\n",
      "[epoch 008/010] step 1300/2250 loss 0.3124\n",
      "[epoch 008/010] step 1350/2250 loss 0.3718\n",
      "[epoch 008/010] step 1400/2250 loss 0.2988\n",
      "[epoch 008/010] step 1450/2250 loss 0.3158\n",
      "[epoch 008/010] step 1500/2250 loss 0.4088\n",
      "[epoch 008/010] step 1550/2250 loss 0.2639\n",
      "[epoch 008/010] step 1600/2250 loss 0.3236\n",
      "[epoch 008/010] step 1650/2250 loss 0.3718\n",
      "[epoch 008/010] step 1700/2250 loss 0.2635\n",
      "[epoch 008/010] step 1750/2250 loss 0.2988\n",
      "[epoch 008/010] step 1800/2250 loss 0.2884\n",
      "[epoch 008/010] step 1850/2250 loss 0.2816\n",
      "[epoch 008/010] step 1900/2250 loss 0.3036\n",
      "[epoch 008/010] step 1950/2250 loss 0.3379\n",
      "[epoch 008/010] step 2000/2250 loss 0.3003\n",
      "[epoch 008/010] step 2050/2250 loss 0.3000\n",
      "[epoch 008/010] step 2100/2250 loss 0.3450\n",
      "[epoch 008/010] step 2150/2250 loss 0.2979\n",
      "[epoch 008/010] step 2200/2250 loss 0.3410\n",
      "[epoch 008/010] step 2250/2250 loss 0.3258\n",
      "epoch 008/010  train=0.3656  val=0.5020\n",
      "[epoch 009/010] step 50/2250 loss 0.3059\n",
      "[epoch 009/010] step 100/2250 loss 0.3730\n",
      "[epoch 009/010] step 150/2250 loss 0.3881\n",
      "[epoch 009/010] step 200/2250 loss 0.2963\n",
      "[epoch 009/010] step 250/2250 loss 0.3153\n",
      "[epoch 009/010] step 300/2250 loss 0.2996\n",
      "[epoch 009/010] step 350/2250 loss 0.3545\n",
      "[epoch 009/010] step 400/2250 loss 0.3726\n",
      "[epoch 009/010] step 450/2250 loss 0.2581\n",
      "[epoch 009/010] step 500/2250 loss 0.3125\n",
      "[epoch 009/010] step 550/2250 loss 0.4127\n",
      "[epoch 009/010] step 600/2250 loss 0.3593\n",
      "[epoch 009/010] step 650/2250 loss 0.3248\n",
      "[epoch 009/010] step 700/2250 loss 0.3001\n",
      "[epoch 009/010] step 750/2250 loss 0.3711\n",
      "[epoch 009/010] step 800/2250 loss 0.4668\n",
      "[epoch 009/010] step 850/2250 loss 0.2939\n",
      "[epoch 009/010] step 900/2250 loss 0.2531\n",
      "[epoch 009/010] step 950/2250 loss 0.3471\n",
      "[epoch 009/010] step 1000/2250 loss 0.3723\n",
      "[epoch 009/010] step 1050/2250 loss 0.2900\n",
      "[epoch 009/010] step 1100/2250 loss 0.3448\n",
      "[epoch 009/010] step 1150/2250 loss 0.3584\n",
      "[epoch 009/010] step 1200/2250 loss 0.3757\n",
      "[epoch 009/010] step 1250/2250 loss 0.2750\n",
      "[epoch 009/010] step 1300/2250 loss 0.3334\n",
      "[epoch 009/010] step 1350/2250 loss 0.3290\n",
      "[epoch 009/010] step 1400/2250 loss 0.2610\n",
      "[epoch 009/010] step 1450/2250 loss 0.3516\n",
      "[epoch 009/010] step 1500/2250 loss 0.3770\n",
      "[epoch 009/010] step 1550/2250 loss 0.2968\n",
      "[epoch 009/010] step 1600/2250 loss 0.3104\n",
      "[epoch 009/010] step 1650/2250 loss 0.4104\n",
      "[epoch 009/010] step 1700/2250 loss 0.3076\n",
      "[epoch 009/010] step 1750/2250 loss 0.2493\n",
      "[epoch 009/010] step 1800/2250 loss 0.3078\n",
      "[epoch 009/010] step 1850/2250 loss 0.3535\n",
      "[epoch 009/010] step 1900/2250 loss 0.3004\n",
      "[epoch 009/010] step 1950/2250 loss 0.2838\n",
      "[epoch 009/010] step 2000/2250 loss 0.2927\n",
      "[epoch 009/010] step 2050/2250 loss 0.3034\n",
      "[epoch 009/010] step 2100/2250 loss 0.2803\n",
      "[epoch 009/010] step 2150/2250 loss 0.3020\n",
      "[epoch 009/010] step 2200/2250 loss 0.3315\n",
      "[epoch 009/010] step 2250/2250 loss 0.2783\n",
      "epoch 009/010  train=0.3623  val=0.5125\n",
      "[epoch 010/010] step 50/2250 loss 0.2933\n",
      "[epoch 010/010] step 100/2250 loss 0.2852\n",
      "[epoch 010/010] step 150/2250 loss 0.3960\n",
      "[epoch 010/010] step 200/2250 loss 0.4047\n",
      "[epoch 010/010] step 250/2250 loss 0.2554\n",
      "[epoch 010/010] step 300/2250 loss 0.3033\n",
      "[epoch 010/010] step 350/2250 loss 0.3982\n",
      "[epoch 010/010] step 400/2250 loss 0.3628\n",
      "[epoch 010/010] step 450/2250 loss 0.3849\n",
      "[epoch 010/010] step 500/2250 loss 0.3412\n",
      "[epoch 010/010] step 550/2250 loss 0.3102\n",
      "[epoch 010/010] step 600/2250 loss 0.2751\n",
      "[epoch 010/010] step 650/2250 loss 0.3284\n",
      "[epoch 010/010] step 700/2250 loss 0.3124\n",
      "[epoch 010/010] step 750/2250 loss 0.2767\n",
      "[epoch 010/010] step 800/2250 loss 0.3221\n",
      "[epoch 010/010] step 850/2250 loss 0.3049\n",
      "[epoch 010/010] step 900/2250 loss 0.3087\n",
      "[epoch 010/010] step 950/2250 loss 0.2983\n",
      "[epoch 010/010] step 1000/2250 loss 0.2725\n",
      "[epoch 010/010] step 1050/2250 loss 0.2748\n",
      "[epoch 010/010] step 1100/2250 loss 0.3885\n",
      "[epoch 010/010] step 1150/2250 loss 0.4428\n",
      "[epoch 010/010] step 1200/2250 loss 0.3642\n",
      "[epoch 010/010] step 1250/2250 loss 0.2592\n",
      "[epoch 010/010] step 1300/2250 loss 0.2792\n",
      "[epoch 010/010] step 1350/2250 loss 0.2974\n",
      "[epoch 010/010] step 1400/2250 loss 0.3064\n",
      "[epoch 010/010] step 1450/2250 loss 0.3562\n",
      "[epoch 010/010] step 1500/2250 loss 0.3718\n",
      "[epoch 010/010] step 1550/2250 loss 0.3977\n",
      "[epoch 010/010] step 1600/2250 loss 0.3540\n",
      "[epoch 010/010] step 1650/2250 loss 0.3483\n",
      "[epoch 010/010] step 1700/2250 loss 0.3815\n",
      "[epoch 010/010] step 1750/2250 loss 0.3252\n",
      "[epoch 010/010] step 1800/2250 loss 0.3789\n",
      "[epoch 010/010] step 1850/2250 loss 0.3607\n",
      "[epoch 010/010] step 1900/2250 loss 0.2775\n",
      "[epoch 010/010] step 1950/2250 loss 0.3788\n",
      "[epoch 010/010] step 2000/2250 loss 0.3397\n",
      "[epoch 010/010] step 2050/2250 loss 0.2940\n",
      "[epoch 010/010] step 2100/2250 loss 0.3065\n",
      "[epoch 010/010] step 2150/2250 loss 0.2659\n",
      "[epoch 010/010] step 2200/2250 loss 0.3573\n",
      "[epoch 010/010] step 2250/2250 loss 0.2512\n",
      "epoch 010/010  train=0.3590  val=0.5270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/18 16:04:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conf = TrainConfig()\n",
    "conf.num_epochs = 10\n",
    "conf.batch_size = 4\n",
    "conf.num_workers = 4\n",
    "conf.lr = 5e-4\n",
    "conf.weight_decay = 1e-4\n",
    "conf.momentum =  0.9\n",
    "\n",
    "\n",
    "def infinite(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "\n",
    "class AlternatingLoader:\n",
    "    def __init__(self, loader_a, loader_b, steps=None, start=\"a\"):\n",
    "        self.a = loader_a\n",
    "        self.b = loader_b\n",
    "        self.steps = steps if steps is not None else 2 * max(len(loader_a), len(loader_b))\n",
    "        self.start = start\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        ia, ib = infinite(self.a), infinite(self.b)\n",
    "        for i in range(self.steps):\n",
    "            if (i % 2 == 0) == (self.start == \"a\"):\n",
    "                yield next(ia)\n",
    "            else:\n",
    "                yield next(ib)\n",
    "\n",
    "dm = DataModule(DataConfig(val_frac=0.1, batch_size=conf.batch_size, num_workers=conf.num_workers), with_masks = False)\n",
    "b_train_loader, b_val_loader = dm.make_loaders_b()\n",
    "\n",
    "a_train_loader = DataLoader(dm.ds_a_train, batch_size=conf.batch_size, shuffle=True, num_workers=conf.num_workers, collate_fn=collate_bb)\n",
    "a_val_loader   = DataLoader(dm.ds_a_val,   batch_size=conf.batch_size, shuffle=False, num_workers=conf.num_workers, collate_fn=collate_bb)\n",
    "\n",
    "mix_train = AlternatingLoader(a_train_loader, b_train_loader)\n",
    "mix_val   = AlternatingLoader(a_val_loader,   b_val_loader)\n",
    "\n",
    "trainer = Trainer(model, conf)\n",
    "hist_ft = trainer.run(mix_train, mix_val, experiment_name=\"Att_FT2_Train6\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f974f7-7f87-476b-aebb-9673010e8a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c98216c0-e163-42d0-b008-f8c66ada5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model.state_dict(), \"../weights/maskrcnn_attfpn30.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276566d-26a3-4892-a51d-da0c794bbea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
