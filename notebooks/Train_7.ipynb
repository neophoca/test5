{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71cdd8a-fc30-46d2-803c-b7ee6eb49cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991aa36b-ed88-41b8-885c-808b77f4962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"file:///media/sdb1/mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "424d6e82-8c48-4970-8a97-703b6385b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from models.models import build_model     \n",
    "from datasets.loader import DataModule, DataConfig  \n",
    "from train.trainer import Trainer, TrainConfig  \n",
    "from train.eval import Evaluator             \n",
    "from datasets.base import collate_bb\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_CLASSES = 1 + 24\n",
    "model_uri = \"file:///media/sdb1/mlflow/753485487056022103/2e19afb3d8e34c7fa8b50505a7dd259e/artifacts/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "\n",
    "def allow_missing_masks(model):\n",
    "    orig_forward = model.forward\n",
    "    def forward(images, targets=None):\n",
    "        if model.training and targets is not None and not all((\"masks\" in t) for t in targets):\n",
    "            rh = model.roi_heads\n",
    "            saved = (rh.mask_roi_pool, rh.mask_head, rh.mask_predictor)\n",
    "            try:\n",
    "                rh.mask_roi_pool, rh.mask_head, rh.mask_predictor = None, None, None\n",
    "                return orig_forward(images, targets)\n",
    "            finally:\n",
    "                rh.mask_roi_pool, rh.mask_head, rh.mask_predictor = saved\n",
    "        return orig_forward(images, targets)\n",
    "    model.forward = forward\n",
    "    return model\n",
    "model = allow_missing_masks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5203de-16bc-4bd5-8917-4f69b533ae68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 001/010] step 50/2250 loss 0.5184\n",
      "[epoch 001/010] step 100/2250 loss 0.4444\n",
      "[epoch 001/010] step 150/2250 loss 0.7280\n",
      "[epoch 001/010] step 200/2250 loss 0.4851\n",
      "[epoch 001/010] step 250/2250 loss 0.5214\n",
      "[epoch 001/010] step 300/2250 loss 0.4425\n",
      "[epoch 001/010] step 350/2250 loss 0.5715\n",
      "[epoch 001/010] step 400/2250 loss 0.4132\n",
      "[epoch 001/010] step 450/2250 loss 0.4593\n",
      "[epoch 001/010] step 500/2250 loss 0.4837\n",
      "[epoch 001/010] step 550/2250 loss 0.4271\n",
      "[epoch 001/010] step 600/2250 loss 0.4830\n",
      "[epoch 001/010] step 650/2250 loss 0.4529\n",
      "[epoch 001/010] step 700/2250 loss 0.4823\n",
      "[epoch 001/010] step 750/2250 loss 0.5911\n",
      "[epoch 001/010] step 800/2250 loss 0.4443\n",
      "[epoch 001/010] step 850/2250 loss 0.4036\n",
      "[epoch 001/010] step 900/2250 loss 0.5260\n",
      "[epoch 001/010] step 950/2250 loss 0.4160\n",
      "[epoch 001/010] step 1000/2250 loss 0.4628\n",
      "[epoch 001/010] step 1050/2250 loss 0.4035\n",
      "[epoch 001/010] step 1100/2250 loss 0.4547\n",
      "[epoch 001/010] step 1150/2250 loss 0.4061\n",
      "[epoch 001/010] step 1200/2250 loss 0.4715\n",
      "[epoch 001/010] step 1250/2250 loss 0.4913\n",
      "[epoch 001/010] step 1300/2250 loss 0.4626\n",
      "[epoch 001/010] step 1350/2250 loss 0.4571\n",
      "[epoch 001/010] step 1400/2250 loss 0.3785\n",
      "[epoch 001/010] step 1450/2250 loss 0.3687\n",
      "[epoch 001/010] step 1500/2250 loss 0.5322\n",
      "[epoch 001/010] step 1550/2250 loss 0.4745\n",
      "[epoch 001/010] step 1600/2250 loss 0.4818\n",
      "[epoch 001/010] step 1650/2250 loss 0.3850\n",
      "[epoch 001/010] step 1700/2250 loss 0.5547\n",
      "[epoch 001/010] step 1750/2250 loss 0.3851\n",
      "[epoch 001/010] step 1800/2250 loss 0.5053\n",
      "[epoch 001/010] step 1850/2250 loss 0.4566\n",
      "[epoch 001/010] step 1900/2250 loss 0.4125\n",
      "[epoch 001/010] step 1950/2250 loss 0.3905\n",
      "[epoch 001/010] step 2000/2250 loss 0.4312\n",
      "[epoch 001/010] step 2050/2250 loss 0.5241\n",
      "[epoch 001/010] step 2100/2250 loss 0.4094\n",
      "[epoch 001/010] step 2150/2250 loss 0.4756\n",
      "[epoch 001/010] step 2200/2250 loss 0.3832\n",
      "[epoch 001/010] step 2250/2250 loss 0.4339\n",
      "epoch 001/010  train=0.4887  val=0.5885\n",
      "[epoch 002/010] step 50/2250 loss 0.4193\n",
      "[epoch 002/010] step 100/2250 loss 0.4067\n",
      "[epoch 002/010] step 150/2250 loss 0.4900\n",
      "[epoch 002/010] step 200/2250 loss 0.4089\n",
      "[epoch 002/010] step 250/2250 loss 0.3242\n",
      "[epoch 002/010] step 300/2250 loss 0.4156\n",
      "[epoch 002/010] step 350/2250 loss 0.3946\n",
      "[epoch 002/010] step 400/2250 loss 0.4641\n",
      "[epoch 002/010] step 450/2250 loss 0.3665\n",
      "[epoch 002/010] step 500/2250 loss 0.3594\n",
      "[epoch 002/010] step 550/2250 loss 0.5168\n",
      "[epoch 002/010] step 600/2250 loss 0.4763\n",
      "[epoch 002/010] step 650/2250 loss 0.5352\n",
      "[epoch 002/010] step 700/2250 loss 0.5167\n",
      "[epoch 002/010] step 750/2250 loss 0.4532\n",
      "[epoch 002/010] step 800/2250 loss 0.5331\n",
      "[epoch 002/010] step 850/2250 loss 0.4451\n",
      "[epoch 002/010] step 900/2250 loss 0.3611\n",
      "[epoch 002/010] step 950/2250 loss 0.4414\n",
      "[epoch 002/010] step 1000/2250 loss 0.3713\n",
      "[epoch 002/010] step 1050/2250 loss 0.4824\n",
      "[epoch 002/010] step 1100/2250 loss 0.4179\n",
      "[epoch 002/010] step 1150/2250 loss 0.5058\n",
      "[epoch 002/010] step 1200/2250 loss 0.4316\n",
      "[epoch 002/010] step 1250/2250 loss 0.4472\n",
      "[epoch 002/010] step 1300/2250 loss 0.4132\n",
      "[epoch 002/010] step 1350/2250 loss 0.5431\n",
      "[epoch 002/010] step 1400/2250 loss 0.3562\n",
      "[epoch 002/010] step 1450/2250 loss 0.3986\n",
      "[epoch 002/010] step 1500/2250 loss 0.3749\n",
      "[epoch 002/010] step 1550/2250 loss 0.4332\n",
      "[epoch 002/010] step 1600/2250 loss 0.4305\n",
      "[epoch 002/010] step 1650/2250 loss 0.3223\n",
      "[epoch 002/010] step 1700/2250 loss 0.3660\n",
      "[epoch 002/010] step 1750/2250 loss 0.5139\n",
      "[epoch 002/010] step 1800/2250 loss 0.4126\n",
      "[epoch 002/010] step 1850/2250 loss 0.4495\n",
      "[epoch 002/010] step 1900/2250 loss 0.3379\n",
      "[epoch 002/010] step 1950/2250 loss 0.3658\n",
      "[epoch 002/010] step 2000/2250 loss 0.5679\n",
      "[epoch 002/010] step 2050/2250 loss 0.5082\n",
      "[epoch 002/010] step 2100/2250 loss 0.4029\n",
      "[epoch 002/010] step 2150/2250 loss 0.3893\n",
      "[epoch 002/010] step 2200/2250 loss 0.5184\n",
      "[epoch 002/010] step 2250/2250 loss 0.4344\n",
      "epoch 002/010  train=0.4653  val=0.5515\n",
      "[epoch 003/010] step 50/2250 loss 0.4565\n",
      "[epoch 003/010] step 100/2250 loss 0.4347\n",
      "[epoch 003/010] step 150/2250 loss 0.5345\n",
      "[epoch 003/010] step 200/2250 loss 0.5153\n",
      "[epoch 003/010] step 250/2250 loss 0.4801\n",
      "[epoch 003/010] step 300/2250 loss 0.3962\n",
      "[epoch 003/010] step 350/2250 loss 0.4334\n",
      "[epoch 003/010] step 400/2250 loss 0.4051\n",
      "[epoch 003/010] step 450/2250 loss 0.3993\n",
      "[epoch 003/010] step 500/2250 loss 0.4742\n",
      "[epoch 003/010] step 550/2250 loss 0.3941\n",
      "[epoch 003/010] step 600/2250 loss 0.3684\n",
      "[epoch 003/010] step 650/2250 loss 0.4730\n",
      "[epoch 003/010] step 700/2250 loss 0.4048\n",
      "[epoch 003/010] step 750/2250 loss 0.4351\n",
      "[epoch 003/010] step 800/2250 loss 0.3989\n",
      "[epoch 003/010] step 850/2250 loss 0.4496\n",
      "[epoch 003/010] step 900/2250 loss 0.4101\n",
      "[epoch 003/010] step 950/2250 loss 0.4267\n",
      "[epoch 003/010] step 1000/2250 loss 0.3403\n",
      "[epoch 003/010] step 1050/2250 loss 0.4259\n",
      "[epoch 003/010] step 1100/2250 loss 0.5090\n",
      "[epoch 003/010] step 1150/2250 loss 0.3306\n",
      "[epoch 003/010] step 1200/2250 loss 0.4211\n",
      "[epoch 003/010] step 1250/2250 loss 0.3964\n",
      "[epoch 003/010] step 1300/2250 loss 0.4285\n",
      "[epoch 003/010] step 1350/2250 loss 0.3551\n",
      "[epoch 003/010] step 1400/2250 loss 0.4347\n",
      "[epoch 003/010] step 1450/2250 loss 0.5482\n",
      "[epoch 003/010] step 1500/2250 loss 0.5134\n",
      "[epoch 003/010] step 1550/2250 loss 0.3505\n",
      "[epoch 003/010] step 1600/2250 loss 0.4822\n",
      "[epoch 003/010] step 1650/2250 loss 0.4793\n",
      "[epoch 003/010] step 1700/2250 loss 0.3902\n",
      "[epoch 003/010] step 1750/2250 loss 0.3692\n",
      "[epoch 003/010] step 1800/2250 loss 0.4649\n",
      "[epoch 003/010] step 1850/2250 loss 0.3833\n",
      "[epoch 003/010] step 1900/2250 loss 0.3867\n",
      "[epoch 003/010] step 1950/2250 loss 0.4756\n",
      "[epoch 003/010] step 2000/2250 loss 0.3486\n",
      "[epoch 003/010] step 2050/2250 loss 0.4405\n",
      "[epoch 003/010] step 2100/2250 loss 0.4009\n",
      "[epoch 003/010] step 2150/2250 loss 0.4339\n",
      "[epoch 003/010] step 2200/2250 loss 0.4048\n",
      "[epoch 003/010] step 2250/2250 loss 0.5121\n",
      "epoch 003/010  train=0.4534  val=0.5469\n",
      "[epoch 004/010] step 50/2250 loss 0.4627\n",
      "[epoch 004/010] step 100/2250 loss 0.4886\n",
      "[epoch 004/010] step 150/2250 loss 0.3898\n",
      "[epoch 004/010] step 200/2250 loss 0.4711\n",
      "[epoch 004/010] step 250/2250 loss 0.3935\n",
      "[epoch 004/010] step 300/2250 loss 0.4254\n",
      "[epoch 004/010] step 350/2250 loss 0.4080\n",
      "[epoch 004/010] step 400/2250 loss 0.3670\n",
      "[epoch 004/010] step 450/2250 loss 0.4289\n",
      "[epoch 004/010] step 500/2250 loss 0.4173\n",
      "[epoch 004/010] step 550/2250 loss 0.3216\n",
      "[epoch 004/010] step 600/2250 loss 0.4439\n",
      "[epoch 004/010] step 650/2250 loss 0.3557\n",
      "[epoch 004/010] step 700/2250 loss 0.4768\n",
      "[epoch 004/010] step 750/2250 loss 0.4450\n",
      "[epoch 004/010] step 800/2250 loss 0.4264\n",
      "[epoch 004/010] step 850/2250 loss 0.3311\n",
      "[epoch 004/010] step 900/2250 loss 0.4079\n",
      "[epoch 004/010] step 950/2250 loss 0.4845\n",
      "[epoch 004/010] step 1000/2250 loss 0.5426\n",
      "[epoch 004/010] step 1050/2250 loss 0.4231\n",
      "[epoch 004/010] step 1100/2250 loss 0.4161\n",
      "[epoch 004/010] step 1150/2250 loss 0.4940\n",
      "[epoch 004/010] step 1200/2250 loss 0.4289\n",
      "[epoch 004/010] step 1250/2250 loss 0.3997\n",
      "[epoch 004/010] step 1300/2250 loss 0.4662\n",
      "[epoch 004/010] step 1350/2250 loss 0.4126\n",
      "[epoch 004/010] step 1400/2250 loss 0.4145\n",
      "[epoch 004/010] step 1450/2250 loss 0.4652\n",
      "[epoch 004/010] step 1500/2250 loss 0.3182\n",
      "[epoch 004/010] step 1550/2250 loss 0.3047\n",
      "[epoch 004/010] step 1600/2250 loss 0.3928\n",
      "[epoch 004/010] step 1650/2250 loss 0.5094\n",
      "[epoch 004/010] step 1700/2250 loss 0.3850\n",
      "[epoch 004/010] step 1750/2250 loss 0.4226\n",
      "[epoch 004/010] step 1800/2250 loss 0.3317\n",
      "[epoch 004/010] step 1850/2250 loss 0.3580\n",
      "[epoch 004/010] step 1900/2250 loss 0.4390\n",
      "[epoch 004/010] step 1950/2250 loss 0.3525\n",
      "[epoch 004/010] step 2000/2250 loss 0.4809\n",
      "[epoch 004/010] step 2050/2250 loss 0.4787\n",
      "[epoch 004/010] step 2100/2250 loss 0.4059\n",
      "[epoch 004/010] step 2150/2250 loss 0.3833\n",
      "[epoch 004/010] step 2200/2250 loss 0.4270\n",
      "[epoch 004/010] step 2250/2250 loss 0.4797\n",
      "epoch 004/010  train=0.4428  val=0.5644\n",
      "[epoch 005/010] step 50/2250 loss 0.3773\n",
      "[epoch 005/010] step 100/2250 loss 0.4787\n",
      "[epoch 005/010] step 150/2250 loss 0.3356\n",
      "[epoch 005/010] step 200/2250 loss 0.4209\n",
      "[epoch 005/010] step 250/2250 loss 0.4389\n"
     ]
    }
   ],
   "source": [
    "conf = TrainConfig()\n",
    "conf.num_epochs = 10\n",
    "conf.batch_size = 4\n",
    "conf.num_workers = 4\n",
    "conf.lr = 5e-4\n",
    "conf.weight_decay = 1e-4\n",
    "conf.momentum =  0.9\n",
    "\n",
    "\n",
    "def infinite(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "\n",
    "class AlternatingLoader:\n",
    "    def __init__(self, loader_a, loader_b, steps=None, start=\"a\"):\n",
    "        self.a = loader_a\n",
    "        self.b = loader_b\n",
    "        self.steps = steps if steps is not None else 2 * max(len(loader_a), len(loader_b))\n",
    "        self.start = start\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        ia, ib = infinite(self.a), infinite(self.b)\n",
    "        for i in range(self.steps):\n",
    "            if (i % 2 == 0) == (self.start == \"a\"):\n",
    "                yield next(ia)\n",
    "            else:\n",
    "                yield next(ib)\n",
    "\n",
    "dm = DataModule(DataConfig(val_frac=0.1, batch_size=conf.batch_size, num_workers=conf.num_workers), with_masks = False)\n",
    "b_train_loader, b_val_loader = dm.make_loaders_b()\n",
    "\n",
    "a_train_loader = DataLoader(dm.ds_a_train, batch_size=conf.batch_size, shuffle=True, num_workers=conf.num_workers, collate_fn=collate_bb)\n",
    "a_val_loader   = DataLoader(dm.ds_a_val,   batch_size=conf.batch_size, shuffle=False, num_workers=conf.num_workers, collate_fn=collate_bb)\n",
    "\n",
    "mix_train = AlternatingLoader(a_train_loader, b_train_loader)\n",
    "mix_val   = AlternatingLoader(a_val_loader,   b_val_loader)\n",
    "\n",
    "trainer = Trainer(model, conf)\n",
    "hist_ft = trainer.run(mix_train, mix_val, experiment_name=\"Att_FT2_Train6\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f974f7-7f87-476b-aebb-9673010e8a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98216c0-e163-42d0-b008-f8c66ada5810",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./weights/maskrcnn_attfpn_warmB_ft_Bmasks_Aboxes.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.save(model.state_dict(), \"./weights/maskrcnn_attfpn_warmB_ft_Bmasks_Aboxes.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
