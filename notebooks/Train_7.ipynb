{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71cdd8a-fc30-46d2-803c-b7ee6eb49cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991aa36b-ed88-41b8-885c-808b77f4962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"file:///media/sdb1/mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "424d6e82-8c48-4970-8a97-703b6385b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from models.models import build_model     \n",
    "from datasets.loader import DataModule, DataConfig  \n",
    "from train.trainer import Trainer, TrainConfig  \n",
    "from train.eval import Evaluator             \n",
    "from datasets.base import collate_bb\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_CLASSES = 1 + 24\n",
    "model_uri = \"file:///media/sdb1/mlflow/753485487056022103/2e19afb3d8e34c7fa8b50505a7dd259e/artifacts/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "\n",
    "def allow_missing_masks(model):\n",
    "    orig_forward = model.forward\n",
    "    def forward(images, targets=None):\n",
    "        if model.training and targets is not None and not all((\"masks\" in t) for t in targets):\n",
    "            rh = model.roi_heads\n",
    "            saved = (rh.mask_roi_pool, rh.mask_head, rh.mask_predictor)\n",
    "            try:\n",
    "                rh.mask_roi_pool, rh.mask_head, rh.mask_predictor = None, None, None\n",
    "                return orig_forward(images, targets)\n",
    "            finally:\n",
    "                rh.mask_roi_pool, rh.mask_head, rh.mask_predictor = saved\n",
    "        return orig_forward(images, targets)\n",
    "    model.forward = forward\n",
    "    return model\n",
    "model = allow_missing_masks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5203de-16bc-4bd5-8917-4f69b533ae68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 001/010] step 50/2250 loss 0.5184\n",
      "[epoch 001/010] step 100/2250 loss 0.4444\n",
      "[epoch 001/010] step 150/2250 loss 0.7280\n",
      "[epoch 001/010] step 200/2250 loss 0.4851\n",
      "[epoch 001/010] step 250/2250 loss 0.5214\n",
      "[epoch 001/010] step 300/2250 loss 0.4425\n",
      "[epoch 001/010] step 350/2250 loss 0.5715\n",
      "[epoch 001/010] step 400/2250 loss 0.4132\n",
      "[epoch 001/010] step 450/2250 loss 0.4593\n",
      "[epoch 001/010] step 500/2250 loss 0.4837\n",
      "[epoch 001/010] step 550/2250 loss 0.4271\n",
      "[epoch 001/010] step 600/2250 loss 0.4830\n",
      "[epoch 001/010] step 650/2250 loss 0.4529\n",
      "[epoch 001/010] step 700/2250 loss 0.4823\n",
      "[epoch 001/010] step 750/2250 loss 0.5911\n",
      "[epoch 001/010] step 800/2250 loss 0.4443\n",
      "[epoch 001/010] step 850/2250 loss 0.4036\n",
      "[epoch 001/010] step 900/2250 loss 0.5260\n",
      "[epoch 001/010] step 950/2250 loss 0.4160\n",
      "[epoch 001/010] step 1000/2250 loss 0.4628\n",
      "[epoch 001/010] step 1050/2250 loss 0.4035\n",
      "[epoch 001/010] step 1100/2250 loss 0.4547\n",
      "[epoch 001/010] step 1150/2250 loss 0.4061\n",
      "[epoch 001/010] step 1200/2250 loss 0.4715\n",
      "[epoch 001/010] step 1250/2250 loss 0.4913\n",
      "[epoch 001/010] step 1300/2250 loss 0.4626\n",
      "[epoch 001/010] step 1350/2250 loss 0.4571\n",
      "[epoch 001/010] step 1400/2250 loss 0.3785\n",
      "[epoch 001/010] step 1450/2250 loss 0.3687\n",
      "[epoch 001/010] step 1500/2250 loss 0.5322\n",
      "[epoch 001/010] step 1550/2250 loss 0.4745\n",
      "[epoch 001/010] step 1600/2250 loss 0.4818\n",
      "[epoch 001/010] step 1650/2250 loss 0.3850\n",
      "[epoch 001/010] step 1700/2250 loss 0.5547\n",
      "[epoch 001/010] step 1750/2250 loss 0.3851\n",
      "[epoch 001/010] step 1800/2250 loss 0.5053\n",
      "[epoch 001/010] step 1850/2250 loss 0.4566\n",
      "[epoch 001/010] step 1900/2250 loss 0.4125\n",
      "[epoch 001/010] step 1950/2250 loss 0.3905\n",
      "[epoch 001/010] step 2000/2250 loss 0.4312\n",
      "[epoch 001/010] step 2050/2250 loss 0.5241\n",
      "[epoch 001/010] step 2100/2250 loss 0.4094\n",
      "[epoch 001/010] step 2150/2250 loss 0.4756\n",
      "[epoch 001/010] step 2200/2250 loss 0.3832\n",
      "[epoch 001/010] step 2250/2250 loss 0.4339\n",
      "epoch 001/010  train=0.4887  val=0.5885\n",
      "[epoch 002/010] step 50/2250 loss 0.4193\n",
      "[epoch 002/010] step 100/2250 loss 0.4067\n",
      "[epoch 002/010] step 150/2250 loss 0.4900\n",
      "[epoch 002/010] step 200/2250 loss 0.4089\n",
      "[epoch 002/010] step 250/2250 loss 0.3242\n",
      "[epoch 002/010] step 300/2250 loss 0.4156\n",
      "[epoch 002/010] step 350/2250 loss 0.3946\n",
      "[epoch 002/010] step 400/2250 loss 0.4641\n",
      "[epoch 002/010] step 450/2250 loss 0.3665\n",
      "[epoch 002/010] step 500/2250 loss 0.3594\n",
      "[epoch 002/010] step 550/2250 loss 0.5168\n",
      "[epoch 002/010] step 600/2250 loss 0.4763\n",
      "[epoch 002/010] step 650/2250 loss 0.5352\n",
      "[epoch 002/010] step 700/2250 loss 0.5167\n",
      "[epoch 002/010] step 750/2250 loss 0.4532\n",
      "[epoch 002/010] step 800/2250 loss 0.5331\n",
      "[epoch 002/010] step 850/2250 loss 0.4451\n",
      "[epoch 002/010] step 900/2250 loss 0.3611\n",
      "[epoch 002/010] step 950/2250 loss 0.4414\n",
      "[epoch 002/010] step 1000/2250 loss 0.3713\n",
      "[epoch 002/010] step 1050/2250 loss 0.4824\n",
      "[epoch 002/010] step 1100/2250 loss 0.4179\n",
      "[epoch 002/010] step 1150/2250 loss 0.5058\n",
      "[epoch 002/010] step 1200/2250 loss 0.4316\n",
      "[epoch 002/010] step 1250/2250 loss 0.4472\n",
      "[epoch 002/010] step 1300/2250 loss 0.4132\n",
      "[epoch 002/010] step 1350/2250 loss 0.5431\n",
      "[epoch 002/010] step 1400/2250 loss 0.3562\n",
      "[epoch 002/010] step 1450/2250 loss 0.3986\n",
      "[epoch 002/010] step 1500/2250 loss 0.3749\n",
      "[epoch 002/010] step 1550/2250 loss 0.4332\n",
      "[epoch 002/010] step 1600/2250 loss 0.4305\n",
      "[epoch 002/010] step 1650/2250 loss 0.3223\n",
      "[epoch 002/010] step 1700/2250 loss 0.3660\n",
      "[epoch 002/010] step 1750/2250 loss 0.5139\n",
      "[epoch 002/010] step 1800/2250 loss 0.4126\n",
      "[epoch 002/010] step 1850/2250 loss 0.4495\n",
      "[epoch 002/010] step 1900/2250 loss 0.3379\n",
      "[epoch 002/010] step 1950/2250 loss 0.3658\n",
      "[epoch 002/010] step 2000/2250 loss 0.5679\n",
      "[epoch 002/010] step 2050/2250 loss 0.5082\n",
      "[epoch 002/010] step 2100/2250 loss 0.4029\n",
      "[epoch 002/010] step 2150/2250 loss 0.3893\n",
      "[epoch 002/010] step 2200/2250 loss 0.5184\n",
      "[epoch 002/010] step 2250/2250 loss 0.4344\n",
      "epoch 002/010  train=0.4653  val=0.5515\n",
      "[epoch 003/010] step 50/2250 loss 0.4565\n",
      "[epoch 003/010] step 100/2250 loss 0.4347\n",
      "[epoch 003/010] step 150/2250 loss 0.5345\n",
      "[epoch 003/010] step 200/2250 loss 0.5153\n",
      "[epoch 003/010] step 250/2250 loss 0.4801\n",
      "[epoch 003/010] step 300/2250 loss 0.3962\n",
      "[epoch 003/010] step 350/2250 loss 0.4334\n",
      "[epoch 003/010] step 400/2250 loss 0.4051\n",
      "[epoch 003/010] step 450/2250 loss 0.3993\n",
      "[epoch 003/010] step 500/2250 loss 0.4742\n",
      "[epoch 003/010] step 550/2250 loss 0.3941\n",
      "[epoch 003/010] step 600/2250 loss 0.3684\n",
      "[epoch 003/010] step 650/2250 loss 0.4730\n",
      "[epoch 003/010] step 700/2250 loss 0.4048\n",
      "[epoch 003/010] step 750/2250 loss 0.4351\n",
      "[epoch 003/010] step 800/2250 loss 0.3989\n",
      "[epoch 003/010] step 850/2250 loss 0.4496\n",
      "[epoch 003/010] step 900/2250 loss 0.4101\n",
      "[epoch 003/010] step 950/2250 loss 0.4267\n",
      "[epoch 003/010] step 1000/2250 loss 0.3403\n",
      "[epoch 003/010] step 1050/2250 loss 0.4259\n",
      "[epoch 003/010] step 1100/2250 loss 0.5090\n",
      "[epoch 003/010] step 1150/2250 loss 0.3306\n",
      "[epoch 003/010] step 1200/2250 loss 0.4211\n",
      "[epoch 003/010] step 1250/2250 loss 0.3964\n",
      "[epoch 003/010] step 1300/2250 loss 0.4285\n",
      "[epoch 003/010] step 1350/2250 loss 0.3551\n",
      "[epoch 003/010] step 1400/2250 loss 0.4347\n",
      "[epoch 003/010] step 1450/2250 loss 0.5482\n",
      "[epoch 003/010] step 1500/2250 loss 0.5134\n",
      "[epoch 003/010] step 1550/2250 loss 0.3505\n",
      "[epoch 003/010] step 1600/2250 loss 0.4822\n",
      "[epoch 003/010] step 1650/2250 loss 0.4793\n",
      "[epoch 003/010] step 1700/2250 loss 0.3902\n",
      "[epoch 003/010] step 1750/2250 loss 0.3692\n",
      "[epoch 003/010] step 1800/2250 loss 0.4649\n",
      "[epoch 003/010] step 1850/2250 loss 0.3833\n",
      "[epoch 003/010] step 1900/2250 loss 0.3867\n",
      "[epoch 003/010] step 1950/2250 loss 0.4756\n",
      "[epoch 003/010] step 2000/2250 loss 0.3486\n",
      "[epoch 003/010] step 2050/2250 loss 0.4405\n",
      "[epoch 003/010] step 2100/2250 loss 0.4009\n",
      "[epoch 003/010] step 2150/2250 loss 0.4339\n",
      "[epoch 003/010] step 2200/2250 loss 0.4048\n",
      "[epoch 003/010] step 2250/2250 loss 0.5121\n",
      "epoch 003/010  train=0.4534  val=0.5469\n",
      "[epoch 004/010] step 50/2250 loss 0.4627\n",
      "[epoch 004/010] step 100/2250 loss 0.4886\n",
      "[epoch 004/010] step 150/2250 loss 0.3898\n",
      "[epoch 004/010] step 200/2250 loss 0.4711\n",
      "[epoch 004/010] step 250/2250 loss 0.3935\n",
      "[epoch 004/010] step 300/2250 loss 0.4254\n",
      "[epoch 004/010] step 350/2250 loss 0.4080\n",
      "[epoch 004/010] step 400/2250 loss 0.3670\n",
      "[epoch 004/010] step 450/2250 loss 0.4289\n",
      "[epoch 004/010] step 500/2250 loss 0.4173\n",
      "[epoch 004/010] step 550/2250 loss 0.3216\n",
      "[epoch 004/010] step 600/2250 loss 0.4439\n",
      "[epoch 004/010] step 650/2250 loss 0.3557\n",
      "[epoch 004/010] step 700/2250 loss 0.4768\n",
      "[epoch 004/010] step 750/2250 loss 0.4450\n",
      "[epoch 004/010] step 800/2250 loss 0.4264\n",
      "[epoch 004/010] step 850/2250 loss 0.3311\n",
      "[epoch 004/010] step 900/2250 loss 0.4079\n",
      "[epoch 004/010] step 950/2250 loss 0.4845\n",
      "[epoch 004/010] step 1000/2250 loss 0.5426\n",
      "[epoch 004/010] step 1050/2250 loss 0.4231\n",
      "[epoch 004/010] step 1100/2250 loss 0.4161\n",
      "[epoch 004/010] step 1150/2250 loss 0.4940\n",
      "[epoch 004/010] step 1200/2250 loss 0.4289\n",
      "[epoch 004/010] step 1250/2250 loss 0.3997\n",
      "[epoch 004/010] step 1300/2250 loss 0.4662\n",
      "[epoch 004/010] step 1350/2250 loss 0.4126\n",
      "[epoch 004/010] step 1400/2250 loss 0.4145\n",
      "[epoch 004/010] step 1450/2250 loss 0.4652\n",
      "[epoch 004/010] step 1500/2250 loss 0.3182\n",
      "[epoch 004/010] step 1550/2250 loss 0.3047\n",
      "[epoch 004/010] step 1600/2250 loss 0.3928\n",
      "[epoch 004/010] step 1650/2250 loss 0.5094\n",
      "[epoch 004/010] step 1700/2250 loss 0.3850\n",
      "[epoch 004/010] step 1750/2250 loss 0.4226\n",
      "[epoch 004/010] step 1800/2250 loss 0.3317\n",
      "[epoch 004/010] step 1850/2250 loss 0.3580\n",
      "[epoch 004/010] step 1900/2250 loss 0.4390\n",
      "[epoch 004/010] step 1950/2250 loss 0.3525\n",
      "[epoch 004/010] step 2000/2250 loss 0.4809\n",
      "[epoch 004/010] step 2050/2250 loss 0.4787\n",
      "[epoch 004/010] step 2100/2250 loss 0.4059\n",
      "[epoch 004/010] step 2150/2250 loss 0.3833\n",
      "[epoch 004/010] step 2200/2250 loss 0.4270\n",
      "[epoch 004/010] step 2250/2250 loss 0.4797\n",
      "epoch 004/010  train=0.4428  val=0.5644\n",
      "[epoch 005/010] step 50/2250 loss 0.3773\n",
      "[epoch 005/010] step 100/2250 loss 0.4787\n",
      "[epoch 005/010] step 150/2250 loss 0.3356\n",
      "[epoch 005/010] step 200/2250 loss 0.4209\n",
      "[epoch 005/010] step 250/2250 loss 0.4389\n",
      "[epoch 005/010] step 300/2250 loss 0.3636\n",
      "[epoch 005/010] step 350/2250 loss 0.3775\n",
      "[epoch 005/010] step 400/2250 loss 0.3238\n",
      "[epoch 005/010] step 450/2250 loss 0.3603\n",
      "[epoch 005/010] step 500/2250 loss 0.3348\n",
      "[epoch 005/010] step 550/2250 loss 0.4997\n",
      "[epoch 005/010] step 600/2250 loss 0.3445\n",
      "[epoch 005/010] step 650/2250 loss 0.3850\n",
      "[epoch 005/010] step 700/2250 loss 0.3874\n",
      "[epoch 005/010] step 750/2250 loss 0.5047\n",
      "[epoch 005/010] step 800/2250 loss 0.4369\n",
      "[epoch 005/010] step 850/2250 loss 0.4004\n",
      "[epoch 005/010] step 900/2250 loss 0.3324\n",
      "[epoch 005/010] step 950/2250 loss 0.3815\n",
      "[epoch 005/010] step 1000/2250 loss 0.3273\n",
      "[epoch 005/010] step 1050/2250 loss 0.3890\n",
      "[epoch 005/010] step 1100/2250 loss 0.3565\n",
      "[epoch 005/010] step 1150/2250 loss 0.5043\n",
      "[epoch 005/010] step 1200/2250 loss 0.3274\n",
      "[epoch 005/010] step 1250/2250 loss 0.3819\n",
      "[epoch 005/010] step 1300/2250 loss 0.3402\n",
      "[epoch 005/010] step 1350/2250 loss 0.3545\n",
      "[epoch 005/010] step 1400/2250 loss 0.3455\n",
      "[epoch 005/010] step 1450/2250 loss 0.4487\n",
      "[epoch 005/010] step 1500/2250 loss 0.3968\n",
      "[epoch 005/010] step 1550/2250 loss 0.3456\n",
      "[epoch 005/010] step 1600/2250 loss 0.4041\n",
      "[epoch 005/010] step 1650/2250 loss 0.2951\n",
      "[epoch 005/010] step 1700/2250 loss 0.4602\n",
      "[epoch 005/010] step 1750/2250 loss 0.5638\n",
      "[epoch 005/010] step 1800/2250 loss 0.3339\n",
      "[epoch 005/010] step 1850/2250 loss 0.3915\n",
      "[epoch 005/010] step 1900/2250 loss 0.4603\n",
      "[epoch 005/010] step 1950/2250 loss 0.3481\n",
      "[epoch 005/010] step 2000/2250 loss 0.3346\n",
      "[epoch 005/010] step 2050/2250 loss 0.4087\n",
      "[epoch 005/010] step 2100/2250 loss 0.4019\n",
      "[epoch 005/010] step 2150/2250 loss 0.4469\n",
      "[epoch 005/010] step 2200/2250 loss 0.4417\n",
      "[epoch 005/010] step 2250/2250 loss 0.5118\n",
      "epoch 005/010  train=0.4331  val=0.5332\n",
      "[epoch 006/010] step 50/2250 loss 0.3273\n",
      "[epoch 006/010] step 100/2250 loss 0.3919\n",
      "[epoch 006/010] step 150/2250 loss 0.3842\n",
      "[epoch 006/010] step 200/2250 loss 0.3478\n",
      "[epoch 006/010] step 250/2250 loss 0.4434\n",
      "[epoch 006/010] step 300/2250 loss 0.4397\n",
      "[epoch 006/010] step 350/2250 loss 0.4353\n",
      "[epoch 006/010] step 400/2250 loss 0.3897\n",
      "[epoch 006/010] step 450/2250 loss 0.4684\n",
      "[epoch 006/010] step 500/2250 loss 0.3908\n",
      "[epoch 006/010] step 550/2250 loss 0.4311\n",
      "[epoch 006/010] step 600/2250 loss 0.3705\n",
      "[epoch 006/010] step 650/2250 loss 0.3801\n",
      "[epoch 006/010] step 700/2250 loss 0.3127\n",
      "[epoch 006/010] step 750/2250 loss 0.4676\n",
      "[epoch 006/010] step 800/2250 loss 0.4043\n",
      "[epoch 006/010] step 850/2250 loss 0.4421\n",
      "[epoch 006/010] step 900/2250 loss 0.3779\n",
      "[epoch 006/010] step 950/2250 loss 0.3465\n",
      "[epoch 006/010] step 1000/2250 loss 0.5057\n",
      "[epoch 006/010] step 1050/2250 loss 0.4132\n",
      "[epoch 006/010] step 1100/2250 loss 0.4232\n",
      "[epoch 006/010] step 1150/2250 loss 0.2959\n",
      "[epoch 006/010] step 1200/2250 loss 0.3807\n",
      "[epoch 006/010] step 1250/2250 loss 0.3785\n",
      "[epoch 006/010] step 1300/2250 loss 0.4236\n",
      "[epoch 006/010] step 1350/2250 loss 0.4023\n",
      "[epoch 006/010] step 1400/2250 loss 0.3662\n",
      "[epoch 006/010] step 1450/2250 loss 0.4114\n",
      "[epoch 006/010] step 1500/2250 loss 0.3975\n",
      "[epoch 006/010] step 1550/2250 loss 0.3621\n",
      "[epoch 006/010] step 1600/2250 loss 0.3996\n",
      "[epoch 006/010] step 1650/2250 loss 0.3386\n",
      "[epoch 006/010] step 1700/2250 loss 0.3546\n",
      "[epoch 006/010] step 1750/2250 loss 0.3157\n",
      "[epoch 006/010] step 1800/2250 loss 0.3708\n",
      "[epoch 006/010] step 1850/2250 loss 0.2833\n",
      "[epoch 006/010] step 1900/2250 loss 0.3630\n",
      "[epoch 006/010] step 1950/2250 loss 0.3232\n",
      "[epoch 006/010] step 2000/2250 loss 0.4261\n",
      "[epoch 006/010] step 2050/2250 loss 0.3389\n",
      "[epoch 006/010] step 2100/2250 loss 0.4249\n",
      "[epoch 006/010] step 2150/2250 loss 0.3560\n",
      "[epoch 006/010] step 2200/2250 loss 0.4989\n",
      "[epoch 006/010] step 2250/2250 loss 0.3409\n",
      "epoch 006/010  train=0.4258  val=0.5531\n",
      "[epoch 007/010] step 50/2250 loss 0.4144\n",
      "[epoch 007/010] step 100/2250 loss 0.4139\n",
      "[epoch 007/010] step 150/2250 loss 0.4986\n",
      "[epoch 007/010] step 200/2250 loss 0.4226\n",
      "[epoch 007/010] step 250/2250 loss 0.2941\n",
      "[epoch 007/010] step 300/2250 loss 0.3578\n",
      "[epoch 007/010] step 350/2250 loss 0.4345\n",
      "[epoch 007/010] step 400/2250 loss 0.3360\n",
      "[epoch 007/010] step 450/2250 loss 0.3417\n",
      "[epoch 007/010] step 500/2250 loss 0.3866\n",
      "[epoch 007/010] step 550/2250 loss 0.3367\n",
      "[epoch 007/010] step 600/2250 loss 0.4518\n",
      "[epoch 007/010] step 650/2250 loss 0.5176\n",
      "[epoch 007/010] step 700/2250 loss 0.3134\n",
      "[epoch 007/010] step 750/2250 loss 0.3668\n",
      "[epoch 007/010] step 800/2250 loss 0.3698\n",
      "[epoch 007/010] step 850/2250 loss 0.3739\n",
      "[epoch 007/010] step 900/2250 loss 0.4523\n",
      "[epoch 007/010] step 950/2250 loss 0.3510\n",
      "[epoch 007/010] step 1000/2250 loss 0.3615\n",
      "[epoch 007/010] step 1050/2250 loss 0.4293\n",
      "[epoch 007/010] step 1100/2250 loss 0.4310\n",
      "[epoch 007/010] step 1150/2250 loss 0.3550\n",
      "[epoch 007/010] step 1200/2250 loss 0.3130\n",
      "[epoch 007/010] step 1250/2250 loss 0.4326\n",
      "[epoch 007/010] step 1300/2250 loss 0.3749\n",
      "[epoch 007/010] step 1350/2250 loss 0.4479\n",
      "[epoch 007/010] step 1400/2250 loss 0.3466\n",
      "[epoch 007/010] step 1450/2250 loss 0.3043\n",
      "[epoch 007/010] step 1500/2250 loss 0.3782\n",
      "[epoch 007/010] step 1550/2250 loss 0.3735\n",
      "[epoch 007/010] step 1600/2250 loss 0.3090\n",
      "[epoch 007/010] step 1650/2250 loss 0.4020\n",
      "[epoch 007/010] step 1700/2250 loss 0.3673\n",
      "[epoch 007/010] step 1750/2250 loss 0.3039\n",
      "[epoch 007/010] step 1800/2250 loss 0.4846\n",
      "[epoch 007/010] step 1850/2250 loss 0.4233\n",
      "[epoch 007/010] step 1900/2250 loss 0.3464\n",
      "[epoch 007/010] step 1950/2250 loss 0.4114\n",
      "[epoch 007/010] step 2000/2250 loss 0.4238\n",
      "[epoch 007/010] step 2050/2250 loss 0.3570\n",
      "[epoch 007/010] step 2100/2250 loss 0.3256\n",
      "[epoch 007/010] step 2150/2250 loss 0.3661\n",
      "[epoch 007/010] step 2200/2250 loss 0.2755\n",
      "[epoch 007/010] step 2250/2250 loss 0.3754\n",
      "epoch 007/010  train=0.4188  val=0.5339\n",
      "[epoch 008/010] step 50/2250 loss 0.4901\n",
      "[epoch 008/010] step 100/2250 loss 0.3851\n",
      "[epoch 008/010] step 150/2250 loss 0.4452\n",
      "[epoch 008/010] step 200/2250 loss 0.4151\n",
      "[epoch 008/010] step 250/2250 loss 0.3555\n",
      "[epoch 008/010] step 300/2250 loss 0.4567\n",
      "[epoch 008/010] step 350/2250 loss 0.3648\n",
      "[epoch 008/010] step 400/2250 loss 0.3431\n",
      "[epoch 008/010] step 450/2250 loss 0.4079\n",
      "[epoch 008/010] step 500/2250 loss 0.4189\n",
      "[epoch 008/010] step 550/2250 loss 0.3656\n",
      "[epoch 008/010] step 600/2250 loss 0.3883\n",
      "[epoch 008/010] step 650/2250 loss 0.3781\n",
      "[epoch 008/010] step 700/2250 loss 0.3934\n",
      "[epoch 008/010] step 750/2250 loss 0.3362\n",
      "[epoch 008/010] step 800/2250 loss 0.4146\n",
      "[epoch 008/010] step 850/2250 loss 0.3760\n",
      "[epoch 008/010] step 900/2250 loss 0.3501\n",
      "[epoch 008/010] step 950/2250 loss 0.4308\n",
      "[epoch 008/010] step 1000/2250 loss 0.4351\n",
      "[epoch 008/010] step 1050/2250 loss 0.3654\n",
      "[epoch 008/010] step 1100/2250 loss 0.4890\n",
      "[epoch 008/010] step 1150/2250 loss 0.3919\n",
      "[epoch 008/010] step 1200/2250 loss 0.4005\n",
      "[epoch 008/010] step 1250/2250 loss 0.3648\n",
      "[epoch 008/010] step 1300/2250 loss 0.4679\n",
      "[epoch 008/010] step 1350/2250 loss 0.3734\n",
      "[epoch 008/010] step 1400/2250 loss 0.4238\n",
      "[epoch 008/010] step 1450/2250 loss 0.3279\n",
      "[epoch 008/010] step 1500/2250 loss 0.3675\n",
      "[epoch 008/010] step 1550/2250 loss 0.4344\n",
      "[epoch 008/010] step 1600/2250 loss 0.3969\n",
      "[epoch 008/010] step 1650/2250 loss 0.3353\n",
      "[epoch 008/010] step 1700/2250 loss 0.3161\n",
      "[epoch 008/010] step 1750/2250 loss 0.3872\n",
      "[epoch 008/010] step 1800/2250 loss 0.4701\n",
      "[epoch 008/010] step 1850/2250 loss 0.3483\n",
      "[epoch 008/010] step 1900/2250 loss 0.3666\n",
      "[epoch 008/010] step 1950/2250 loss 0.3966\n",
      "[epoch 008/010] step 2000/2250 loss 0.4335\n",
      "[epoch 008/010] step 2050/2250 loss 0.3320\n",
      "[epoch 008/010] step 2100/2250 loss 0.4074\n",
      "[epoch 008/010] step 2150/2250 loss 0.3811\n",
      "[epoch 008/010] step 2200/2250 loss 0.3122\n",
      "[epoch 008/010] step 2250/2250 loss 0.3942\n",
      "epoch 008/010  train=0.4125  val=0.5287\n",
      "[epoch 009/010] step 50/2250 loss 0.3329\n",
      "[epoch 009/010] step 100/2250 loss 0.3059\n",
      "[epoch 009/010] step 150/2250 loss 0.3935\n",
      "[epoch 009/010] step 200/2250 loss 0.3200\n",
      "[epoch 009/010] step 250/2250 loss 0.3736\n",
      "[epoch 009/010] step 300/2250 loss 0.3080\n",
      "[epoch 009/010] step 350/2250 loss 0.4217\n",
      "[epoch 009/010] step 400/2250 loss 0.3232\n",
      "[epoch 009/010] step 450/2250 loss 0.3748\n",
      "[epoch 009/010] step 500/2250 loss 0.4382\n",
      "[epoch 009/010] step 550/2250 loss 0.2767\n",
      "[epoch 009/010] step 600/2250 loss 0.3543\n",
      "[epoch 009/010] step 650/2250 loss 0.3909\n",
      "[epoch 009/010] step 700/2250 loss 0.3750\n",
      "[epoch 009/010] step 750/2250 loss 0.3649\n",
      "[epoch 009/010] step 800/2250 loss 0.3004\n",
      "[epoch 009/010] step 850/2250 loss 0.4022\n",
      "[epoch 009/010] step 900/2250 loss 0.4103\n",
      "[epoch 009/010] step 950/2250 loss 0.3474\n",
      "[epoch 009/010] step 1000/2250 loss 0.3743\n",
      "[epoch 009/010] step 1050/2250 loss 0.3379\n",
      "[epoch 009/010] step 1100/2250 loss 0.3051\n",
      "[epoch 009/010] step 1150/2250 loss 0.3767\n",
      "[epoch 009/010] step 1200/2250 loss 0.3193\n",
      "[epoch 009/010] step 1250/2250 loss 0.2964\n",
      "[epoch 009/010] step 1300/2250 loss 0.3577\n",
      "[epoch 009/010] step 1350/2250 loss 0.3475\n",
      "[epoch 009/010] step 1400/2250 loss 0.3368\n",
      "[epoch 009/010] step 1450/2250 loss 0.3684\n",
      "[epoch 009/010] step 1500/2250 loss 0.4160\n",
      "[epoch 009/010] step 1550/2250 loss 0.3347\n",
      "[epoch 009/010] step 1600/2250 loss 0.5164\n",
      "[epoch 009/010] step 1650/2250 loss 0.3605\n",
      "[epoch 009/010] step 1700/2250 loss 0.3030\n",
      "[epoch 009/010] step 1750/2250 loss 0.3503\n",
      "[epoch 009/010] step 1800/2250 loss 0.3309\n",
      "[epoch 009/010] step 1850/2250 loss 0.4569\n",
      "[epoch 009/010] step 1900/2250 loss 0.3666\n",
      "[epoch 009/010] step 1950/2250 loss 0.2923\n",
      "[epoch 009/010] step 2000/2250 loss 0.4116\n",
      "[epoch 009/010] step 2050/2250 loss 0.3269\n",
      "[epoch 009/010] step 2100/2250 loss 0.3088\n",
      "[epoch 009/010] step 2150/2250 loss 0.2850\n",
      "[epoch 009/010] step 2200/2250 loss 0.3983\n",
      "[epoch 009/010] step 2250/2250 loss 0.3954\n",
      "epoch 009/010  train=0.4058  val=0.5313\n",
      "[epoch 010/010] step 50/2250 loss 0.3027\n",
      "[epoch 010/010] step 100/2250 loss 0.4037\n",
      "[epoch 010/010] step 150/2250 loss 0.3014\n",
      "[epoch 010/010] step 200/2250 loss 0.3783\n",
      "[epoch 010/010] step 250/2250 loss 0.4591\n",
      "[epoch 010/010] step 300/2250 loss 0.3489\n",
      "[epoch 010/010] step 350/2250 loss 0.3489\n",
      "[epoch 010/010] step 400/2250 loss 0.3827\n",
      "[epoch 010/010] step 450/2250 loss 0.4032\n",
      "[epoch 010/010] step 500/2250 loss 0.3520\n",
      "[epoch 010/010] step 550/2250 loss 0.3770\n",
      "[epoch 010/010] step 600/2250 loss 0.3667\n",
      "[epoch 010/010] step 650/2250 loss 0.3760\n",
      "[epoch 010/010] step 700/2250 loss 0.4068\n",
      "[epoch 010/010] step 750/2250 loss 0.4317\n",
      "[epoch 010/010] step 800/2250 loss 0.3490\n",
      "[epoch 010/010] step 850/2250 loss 0.3506\n",
      "[epoch 010/010] step 900/2250 loss 0.3558\n",
      "[epoch 010/010] step 950/2250 loss 0.4115\n",
      "[epoch 010/010] step 1000/2250 loss 0.3304\n",
      "[epoch 010/010] step 1050/2250 loss 0.2972\n",
      "[epoch 010/010] step 1100/2250 loss 0.3486\n",
      "[epoch 010/010] step 1150/2250 loss 0.3588\n",
      "[epoch 010/010] step 1200/2250 loss 0.3552\n",
      "[epoch 010/010] step 1250/2250 loss 0.3142\n",
      "[epoch 010/010] step 1300/2250 loss 0.3366\n",
      "[epoch 010/010] step 1350/2250 loss 0.3162\n",
      "[epoch 010/010] step 1400/2250 loss 0.3730\n",
      "[epoch 010/010] step 1450/2250 loss 0.4208\n",
      "[epoch 010/010] step 1500/2250 loss 0.2615\n",
      "[epoch 010/010] step 1550/2250 loss 0.3241\n",
      "[epoch 010/010] step 1600/2250 loss 0.3670\n",
      "[epoch 010/010] step 1650/2250 loss 0.3515\n",
      "[epoch 010/010] step 1700/2250 loss 0.3048\n",
      "[epoch 010/010] step 1750/2250 loss 0.3822\n",
      "[epoch 010/010] step 1800/2250 loss 0.3494\n",
      "[epoch 010/010] step 1850/2250 loss 0.3723\n",
      "[epoch 010/010] step 1900/2250 loss 0.3402\n",
      "[epoch 010/010] step 1950/2250 loss 0.2951\n",
      "[epoch 010/010] step 2000/2250 loss 0.4666\n",
      "[epoch 010/010] step 2050/2250 loss 0.3307\n",
      "[epoch 010/010] step 2100/2250 loss 0.3304\n",
      "[epoch 010/010] step 2150/2250 loss 0.3651\n",
      "[epoch 010/010] step 2200/2250 loss 0.3809\n",
      "[epoch 010/010] step 2250/2250 loss 0.3273\n",
      "epoch 010/010  train=0.4011  val=0.5296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/18 09:46:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conf = TrainConfig()\n",
    "conf.num_epochs = 10\n",
    "conf.batch_size = 4\n",
    "conf.num_workers = 4\n",
    "conf.lr = 5e-4\n",
    "conf.weight_decay = 1e-4\n",
    "conf.momentum =  0.9\n",
    "\n",
    "\n",
    "def infinite(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "\n",
    "class AlternatingLoader:\n",
    "    def __init__(self, loader_a, loader_b, steps=None, start=\"a\"):\n",
    "        self.a = loader_a\n",
    "        self.b = loader_b\n",
    "        self.steps = steps if steps is not None else 2 * max(len(loader_a), len(loader_b))\n",
    "        self.start = start\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        ia, ib = infinite(self.a), infinite(self.b)\n",
    "        for i in range(self.steps):\n",
    "            if (i % 2 == 0) == (self.start == \"a\"):\n",
    "                yield next(ia)\n",
    "            else:\n",
    "                yield next(ib)\n",
    "\n",
    "dm = DataModule(DataConfig(val_frac=0.1, batch_size=conf.batch_size, num_workers=conf.num_workers), with_masks = False)\n",
    "b_train_loader, b_val_loader = dm.make_loaders_b()\n",
    "\n",
    "a_train_loader = DataLoader(dm.ds_a_train, batch_size=conf.batch_size, shuffle=True, num_workers=conf.num_workers, collate_fn=collate_bb)\n",
    "a_val_loader   = DataLoader(dm.ds_a_val,   batch_size=conf.batch_size, shuffle=False, num_workers=conf.num_workers, collate_fn=collate_bb)\n",
    "\n",
    "mix_train = AlternatingLoader(a_train_loader, b_train_loader)\n",
    "mix_val   = AlternatingLoader(a_val_loader,   b_val_loader)\n",
    "\n",
    "trainer = Trainer(model, conf)\n",
    "hist_ft = trainer.run(mix_train, mix_val, experiment_name=\"Att_FT2_Train6\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f974f7-7f87-476b-aebb-9673010e8a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98216c0-e163-42d0-b008-f8c66ada5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model.state_dict(), \"../weights/maskrcnn_attfpn20.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276566d-26a3-4892-a51d-da0c794bbea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
