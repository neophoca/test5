{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d528c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a7158",
   "metadata": {},
   "source": [
    "# Train A → B (classic 2-stage)\n",
    "\n",
    "Stage 1: pretrain on **Dataset A (boxes)**.  \n",
    "Stage 2: fine-tune on **Dataset B (masks)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577e080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from models.models import build_model2\n",
    "from datasets import cfg\n",
    "from datasets.loader import DataModule, DataConfig\n",
    "from train.trainer_v2 import Trainer, TrainConfig\n",
    "from train.eval import Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2813bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"maskrcnn_attfpn\"  \n",
    "NUM_CLASSES = cfg.num_classes    # 1 + 24\n",
    "\n",
    "# If True, DS A will include rectangle masks (weak masks derived from boxes).\n",
    "PRETRAIN_WITH_WEAK_MASKS = True #let's just follow the paper whatever\n",
    "\n",
    "TRACKING_URI = \"file:///media/sdb1/mlflow\"\n",
    "EXPERIMENT_PRE_A = \"AB_classic_preA\"\n",
    "EXPERIMENT_FT_B  = \"AB_classic_ftB\"\n",
    "\n",
    "WEIGHTS_DIR = Path(\"../weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c424e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 3370 train | 374 val\n",
      "B: 475 train | 52 val\n"
     ]
    }
   ],
   "source": [
    "dm = DataModule(DataConfig(val_frac=0.1, batch_size=4, num_workers=4),num_channels=3,with_masks=bool(PRETRAIN_WITH_WEAK_MASKS),) #B always has masks, A produces rectangle masks if we ask for them \n",
    "\n",
    "a_train, a_val = dm.make_loaders_a()\n",
    "b_train, b_val = dm.make_loaders_b()\n",
    "\n",
    "print(\"A:\", len(dm.ds_a_train), \"train |\", len(dm.ds_a_val), \"val\") #we split train part into train & val. there is also test part. \n",
    "print(\"B:\", len(dm.ds_b_train), \"train |\", len(dm.ds_b_val), \"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "513338d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = build_model2(MODEL_NAME, NUM_CLASSES,weights_backbone=False,trainable_backbone_layers=5).to(DEVICE) #resnet 50 imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f814d",
   "metadata": {},
   "source": [
    "## Stage 1 — pretrain on DS A (boxes)\n",
    "\n",
    "If `PRETRAIN_WITH_WEAK_MASKS=True`, DS A provides rectangle masks so the mask head also sees a weak signal.\n",
    "Otherwise, the trainer automatically skips the mask head when masks are missing (detection-only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f371ab09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neoph/dev/Train/Jupyter/KaryoTest/train/trainer_v2.py:117: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=bool(train_conf.amp and self.device.type == \"cuda\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 001/020] step 50/843 loss 15.6025\n",
      "[epoch 001/020] step 100/843 loss 2.8024\n",
      "[epoch 001/020] step 150/843 loss 2.6432\n",
      "[epoch 001/020] step 200/843 loss 3.3272\n",
      "[epoch 001/020] step 250/843 loss 3.0136\n",
      "[epoch 001/020] step 300/843 loss 2.7734\n",
      "[epoch 001/020] step 350/843 loss 3.2292\n",
      "[epoch 001/020] step 400/843 loss 3.1817\n",
      "[epoch 001/020] step 450/843 loss 3.1564\n",
      "[epoch 001/020] step 500/843 loss 3.0853\n",
      "[epoch 001/020] step 550/843 loss 3.1200\n",
      "[epoch 001/020] step 600/843 loss 3.1480\n",
      "[epoch 001/020] step 650/843 loss 3.1927\n",
      "[epoch 001/020] step 700/843 loss 3.0340\n",
      "[epoch 001/020] step 750/843 loss 3.2251\n",
      "[epoch 001/020] step 800/843 loss 3.0389\n",
      "[epoch 001/020] step 843/843 loss 3.6312\n",
      "[epoch 001/020] train=5.5617  val=6.7116  lr=0.004215\n",
      "[epoch 002/020] step 50/843 loss 3.1306\n",
      "[epoch 002/020] step 100/843 loss 2.9910\n",
      "[epoch 002/020] step 150/843 loss 2.9539\n",
      "[epoch 002/020] step 200/843 loss 3.0815\n",
      "[epoch 002/020] step 250/843 loss 2.9441\n",
      "[epoch 002/020] step 300/843 loss 3.0405\n",
      "[epoch 002/020] step 350/843 loss 3.0691\n",
      "[epoch 002/020] step 400/843 loss 2.9782\n",
      "[epoch 002/020] step 450/843 loss 2.8077\n",
      "[epoch 002/020] step 500/843 loss 2.8219\n",
      "[epoch 002/020] step 550/843 loss 2.7743\n",
      "[epoch 002/020] step 600/843 loss 2.8762\n",
      "[epoch 002/020] step 650/843 loss 2.9464\n",
      "[epoch 002/020] step 700/843 loss 2.7713\n",
      "[epoch 002/020] step 750/843 loss 2.7228\n",
      "[epoch 002/020] step 800/843 loss 2.6979\n",
      "[epoch 002/020] step 843/843 loss 2.5878\n",
      "[epoch 002/020] train=2.9248  val=3.1853  lr=0.00497696\n",
      "[epoch 003/020] step 50/843 loss 2.7555\n",
      "[epoch 003/020] step 100/843 loss 2.7997\n",
      "[epoch 003/020] step 150/843 loss 2.5478\n",
      "[epoch 003/020] step 200/843 loss 2.5976\n",
      "[epoch 003/020] step 250/843 loss 2.6497\n",
      "[epoch 003/020] step 300/843 loss 2.6570\n",
      "[epoch 003/020] step 350/843 loss 2.4157\n",
      "[epoch 003/020] step 400/843 loss 2.4344\n",
      "[epoch 003/020] step 450/843 loss 2.4768\n",
      "[epoch 003/020] step 500/843 loss 2.4689\n",
      "[epoch 003/020] step 550/843 loss 2.5151\n",
      "[epoch 003/020] step 600/843 loss 2.4499\n",
      "[epoch 003/020] step 650/843 loss 2.4271\n",
      "[epoch 003/020] step 700/843 loss 2.5243\n",
      "[epoch 003/020] step 750/843 loss 2.4113\n",
      "[epoch 003/020] step 800/843 loss 2.5160\n",
      "[epoch 003/020] step 843/843 loss 2.4274\n",
      "[epoch 003/020] train=2.5513  val=2.6900  lr=0.00488623\n",
      "[epoch 004/020] step 50/843 loss 2.4524\n",
      "[epoch 004/020] step 100/843 loss 2.3789\n",
      "[epoch 004/020] step 150/843 loss 2.3977\n",
      "[epoch 004/020] step 200/843 loss 2.3007\n",
      "[epoch 004/020] step 250/843 loss 2.2976\n",
      "[epoch 004/020] step 300/843 loss 2.2042\n",
      "[epoch 004/020] step 350/843 loss 2.2190\n",
      "[epoch 004/020] step 400/843 loss 2.2367\n",
      "[epoch 004/020] step 450/843 loss 2.3146\n",
      "[epoch 004/020] step 500/843 loss 2.1110\n",
      "[epoch 004/020] step 550/843 loss 2.3242\n",
      "[epoch 004/020] step 600/843 loss 2.0450\n",
      "[epoch 004/020] step 650/843 loss 2.3215\n",
      "[epoch 004/020] step 700/843 loss 2.2404\n",
      "[epoch 004/020] step 750/843 loss 2.2147\n",
      "[epoch 004/020] step 800/843 loss 2.1245\n",
      "[epoch 004/020] step 843/843 loss 2.5986\n",
      "[epoch 004/020] train=2.2897  val=2.3969  lr=0.00472914\n",
      "[epoch 005/020] step 50/843 loss 2.1786\n",
      "[epoch 005/020] step 100/843 loss 2.2998\n",
      "[epoch 005/020] step 150/843 loss 2.0853\n",
      "[epoch 005/020] step 200/843 loss 2.0663\n",
      "[epoch 005/020] step 250/843 loss 2.0519\n",
      "[epoch 005/020] step 300/843 loss 2.0683\n",
      "[epoch 005/020] step 350/843 loss 2.0676\n",
      "[epoch 005/020] step 400/843 loss 2.1492\n",
      "[epoch 005/020] step 450/843 loss 1.9723\n",
      "[epoch 005/020] step 500/843 loss 1.8377\n",
      "[epoch 005/020] step 550/843 loss 2.1228\n",
      "[epoch 005/020] step 600/843 loss 2.0748\n",
      "[epoch 005/020] step 650/843 loss 2.0727\n",
      "[epoch 005/020] step 700/843 loss 2.0380\n",
      "[epoch 005/020] step 750/843 loss 2.0918\n",
      "[epoch 005/020] step 800/843 loss 2.0437\n",
      "[epoch 005/020] step 843/843 loss 2.0491\n",
      "[epoch 005/020] train=2.1263  val=2.1756  lr=0.00451005\n",
      "[epoch 006/020] step 50/843 loss 1.9606\n",
      "[epoch 006/020] step 100/843 loss 1.8943\n",
      "[epoch 006/020] step 150/843 loss 1.9755\n",
      "[epoch 006/020] step 200/843 loss 1.7928\n",
      "[epoch 006/020] step 250/843 loss 2.0090\n",
      "[epoch 006/020] step 300/843 loss 2.0003\n",
      "[epoch 006/020] step 350/843 loss 1.9476\n",
      "[epoch 006/020] step 400/843 loss 2.0837\n",
      "[epoch 006/020] step 450/843 loss 1.9683\n",
      "[epoch 006/020] step 500/843 loss 1.9144\n",
      "[epoch 006/020] step 550/843 loss 2.2191\n",
      "[epoch 006/020] step 600/843 loss 2.1444\n",
      "[epoch 006/020] step 650/843 loss 1.6825\n",
      "[epoch 006/020] step 700/843 loss 1.9080\n",
      "[epoch 006/020] step 750/843 loss 1.9712\n",
      "[epoch 006/020] step 800/843 loss 1.9129\n",
      "[epoch 006/020] step 843/843 loss 2.0467\n",
      "[epoch 006/020] train=2.0101  val=2.0212  lr=0.00423506\n",
      "[epoch 007/020] step 50/843 loss 1.9594\n",
      "[epoch 007/020] step 100/843 loss 1.9261\n",
      "[epoch 007/020] step 150/843 loss 1.9467\n",
      "[epoch 007/020] step 200/843 loss 2.0250\n",
      "[epoch 007/020] step 250/843 loss 1.8464\n",
      "[epoch 007/020] step 300/843 loss 1.7976\n",
      "[epoch 007/020] step 350/843 loss 2.0484\n",
      "[epoch 007/020] step 400/843 loss 1.8844\n",
      "[epoch 007/020] step 450/843 loss 1.9378\n",
      "[epoch 007/020] step 500/843 loss 1.9934\n",
      "[epoch 007/020] step 550/843 loss 1.7920\n",
      "[epoch 007/020] step 600/843 loss 1.8312\n",
      "[epoch 007/020] step 650/843 loss 1.9541\n",
      "[epoch 007/020] step 700/843 loss 1.8436\n",
      "[epoch 007/020] step 750/843 loss 1.7549\n",
      "[epoch 007/020] step 800/843 loss 1.8293\n",
      "[epoch 007/020] step 843/843 loss 1.7465\n",
      "[epoch 007/020] train=1.9146  val=1.9074  lr=0.00391181\n",
      "[epoch 008/020] step 50/843 loss 1.8574\n",
      "[epoch 008/020] step 100/843 loss 2.0244\n",
      "[epoch 008/020] step 150/843 loss 2.1938\n",
      "[epoch 008/020] step 200/843 loss 1.9756\n",
      "[epoch 008/020] step 250/843 loss 1.9125\n",
      "[epoch 008/020] step 300/843 loss 1.7721\n",
      "[epoch 008/020] step 350/843 loss 2.0024\n",
      "[epoch 008/020] step 400/843 loss 1.7897\n",
      "[epoch 008/020] step 450/843 loss 1.8733\n",
      "[epoch 008/020] step 500/843 loss 1.8073\n",
      "[epoch 008/020] step 550/843 loss 1.9611\n",
      "[epoch 008/020] step 600/843 loss 1.8750\n",
      "[epoch 008/020] step 650/843 loss 1.6605\n",
      "[epoch 008/020] step 700/843 loss 1.7273\n",
      "[epoch 008/020] step 750/843 loss 1.6574\n",
      "[epoch 008/020] step 800/843 loss 1.8764\n",
      "[epoch 008/020] step 843/843 loss 1.9459\n",
      "[epoch 008/020] train=1.8442  val=1.8339  lr=0.0035493\n",
      "[epoch 009/020] step 50/843 loss 2.0702\n",
      "[epoch 009/020] step 100/843 loss 1.8132\n",
      "[epoch 009/020] step 150/843 loss 2.0839\n",
      "[epoch 009/020] step 200/843 loss 1.8456\n",
      "[epoch 009/020] step 250/843 loss 1.7131\n",
      "[epoch 009/020] step 300/843 loss 1.6972\n",
      "[epoch 009/020] step 350/843 loss 1.6529\n",
      "[epoch 009/020] step 400/843 loss 1.6927\n",
      "[epoch 009/020] step 450/843 loss 1.6841\n",
      "[epoch 009/020] step 500/843 loss 1.7292\n",
      "[epoch 009/020] step 550/843 loss 1.8349\n",
      "[epoch 009/020] step 600/843 loss 1.8157\n",
      "[epoch 009/020] step 650/843 loss 1.6385\n",
      "[epoch 009/020] step 700/843 loss 1.7632\n",
      "[epoch 009/020] step 750/843 loss 1.9667\n",
      "[epoch 009/020] step 800/843 loss 1.9013\n",
      "[epoch 009/020] step 843/843 loss 1.7046\n",
      "[epoch 009/020] train=1.7895  val=1.7614  lr=0.00315761\n",
      "[epoch 010/020] step 50/843 loss 1.6179\n",
      "[epoch 010/020] step 100/843 loss 1.6836\n",
      "[epoch 010/020] step 150/843 loss 1.6878\n",
      "[epoch 010/020] step 200/843 loss 1.7104\n",
      "[epoch 010/020] step 250/843 loss 1.7439\n",
      "[epoch 010/020] step 300/843 loss 1.8194\n",
      "[epoch 010/020] step 350/843 loss 1.6269\n",
      "[epoch 010/020] step 400/843 loss 1.8670\n",
      "[epoch 010/020] step 450/843 loss 1.8775\n",
      "[epoch 010/020] step 500/843 loss 1.7087\n",
      "[epoch 010/020] step 550/843 loss 1.8031\n",
      "[epoch 010/020] step 600/843 loss 1.7627\n",
      "[epoch 010/020] step 650/843 loss 1.7696\n",
      "[epoch 010/020] step 700/843 loss 1.7676\n",
      "[epoch 010/020] step 750/843 loss 1.7841\n",
      "[epoch 010/020] step 800/843 loss 1.8378\n",
      "[epoch 010/020] step 843/843 loss 1.7090\n",
      "[epoch 010/020] train=1.7358  val=1.7167  lr=0.00274765\n",
      "[epoch 011/020] step 50/843 loss 1.8574\n",
      "[epoch 011/020] step 100/843 loss 1.7555\n",
      "[epoch 011/020] step 150/843 loss 1.7842\n",
      "[epoch 011/020] step 200/843 loss 1.5735\n",
      "[epoch 011/020] step 250/843 loss 1.6719\n",
      "[epoch 011/020] step 300/843 loss 1.7911\n",
      "[epoch 011/020] step 350/843 loss 1.8577\n",
      "[epoch 011/020] step 400/843 loss 1.7528\n",
      "[epoch 011/020] step 450/843 loss 1.5966\n",
      "[epoch 011/020] step 500/843 loss 1.6726\n",
      "[epoch 011/020] step 550/843 loss 1.7814\n",
      "[epoch 011/020] step 600/843 loss 1.6917\n",
      "[epoch 011/020] step 650/843 loss 1.6656\n",
      "[epoch 011/020] step 700/843 loss 1.7700\n",
      "[epoch 011/020] step 750/843 loss 1.5773\n",
      "[epoch 011/020] step 800/843 loss 1.6470\n",
      "[epoch 011/020] step 843/843 loss 1.5417\n",
      "[epoch 011/020] train=1.6996  val=1.6795  lr=0.00233081\n",
      "[epoch 012/020] step 50/843 loss 1.7490\n",
      "[epoch 012/020] step 100/843 loss 1.6435\n",
      "[epoch 012/020] step 150/843 loss 1.5606\n",
      "[epoch 012/020] step 200/843 loss 1.8497\n",
      "[epoch 012/020] step 250/843 loss 1.8012\n",
      "[epoch 012/020] step 300/843 loss 1.6784\n",
      "[epoch 012/020] step 350/843 loss 1.4780\n",
      "[epoch 012/020] step 400/843 loss 1.6442\n",
      "[epoch 012/020] step 450/843 loss 1.5683\n",
      "[epoch 012/020] step 500/843 loss 1.6720\n",
      "[epoch 012/020] step 550/843 loss 1.6515\n",
      "[epoch 012/020] step 600/843 loss 1.7310\n",
      "[epoch 012/020] step 650/843 loss 1.6525\n",
      "[epoch 012/020] step 700/843 loss 1.5971\n",
      "[epoch 012/020] step 750/843 loss 1.6118\n",
      "[epoch 012/020] step 800/843 loss 1.7227\n",
      "[epoch 012/020] step 843/843 loss 1.7184\n",
      "[epoch 012/020] train=1.6681  val=1.6412  lr=0.00191869\n",
      "[epoch 013/020] step 50/843 loss 1.5467\n",
      "[epoch 013/020] step 100/843 loss 1.6328\n",
      "[epoch 013/020] step 150/843 loss 1.5454\n",
      "[epoch 013/020] step 200/843 loss 1.5340\n",
      "[epoch 013/020] step 250/843 loss 1.6415\n",
      "[epoch 013/020] step 300/843 loss 1.7299\n",
      "[epoch 013/020] step 350/843 loss 1.5763\n",
      "[epoch 013/020] step 400/843 loss 1.6177\n",
      "[epoch 013/020] step 450/843 loss 1.4196\n",
      "[epoch 013/020] step 500/843 loss 1.4948\n",
      "[epoch 013/020] step 550/843 loss 1.7606\n",
      "[epoch 013/020] step 600/843 loss 1.8283\n",
      "[epoch 013/020] step 650/843 loss 1.7893\n",
      "[epoch 013/020] step 700/843 loss 1.6635\n",
      "[epoch 013/020] step 750/843 loss 1.7270\n",
      "[epoch 013/020] step 800/843 loss 1.5207\n",
      "[epoch 013/020] step 843/843 loss 2.1700\n",
      "[epoch 013/020] train=1.6354  val=1.6150  lr=0.00152275\n",
      "[epoch 014/020] step 50/843 loss 1.5641\n",
      "[epoch 014/020] step 100/843 loss 1.6630\n",
      "[epoch 014/020] step 150/843 loss 1.5837\n",
      "[epoch 014/020] step 200/843 loss 1.7623\n",
      "[epoch 014/020] step 250/843 loss 1.5718\n",
      "[epoch 014/020] step 300/843 loss 1.6992\n",
      "[epoch 014/020] step 400/843 loss 1.6491\n",
      "[epoch 014/020] step 450/843 loss 1.6550\n",
      "[epoch 014/020] step 500/843 loss 1.6711\n",
      "[epoch 014/020] step 550/843 loss 1.7176\n",
      "[epoch 014/020] step 600/843 loss 1.6692\n",
      "[epoch 014/020] step 650/843 loss 1.5359\n",
      "[epoch 014/020] step 700/843 loss 1.6055\n",
      "[epoch 014/020] step 750/843 loss 1.7317\n",
      "[epoch 014/020] step 800/843 loss 1.5803\n",
      "[epoch 014/020] step 843/843 loss 1.4381\n",
      "[epoch 014/020] train=1.6105  val=1.5954  lr=0.00115402\n",
      "[epoch 015/020] step 50/843 loss 1.6330\n",
      "[epoch 015/020] step 100/843 loss 1.6585\n",
      "[epoch 015/020] step 150/843 loss 1.5202\n",
      "[epoch 015/020] step 200/843 loss 1.3852\n",
      "[epoch 015/020] step 250/843 loss 1.6382\n",
      "[epoch 015/020] step 300/843 loss 1.5837\n",
      "[epoch 015/020] step 350/843 loss 1.5677\n",
      "[epoch 015/020] step 400/843 loss 1.8186\n",
      "[epoch 015/020] step 450/843 loss 1.5606\n",
      "[epoch 015/020] step 500/843 loss 1.4892\n",
      "[epoch 015/020] step 550/843 loss 1.7115\n",
      "[epoch 015/020] step 600/843 loss 1.4118\n",
      "[epoch 015/020] step 650/843 loss 1.5154\n",
      "[epoch 015/020] step 700/843 loss 1.4689\n",
      "[epoch 015/020] step 750/843 loss 1.6502\n",
      "[epoch 015/020] step 800/843 loss 1.5377\n",
      "[epoch 015/020] step 843/843 loss 1.6012\n",
      "[epoch 015/020] train=1.5907  val=1.5848  lr=0.000822741\n",
      "[epoch 016/020] step 50/843 loss 1.7278\n",
      "[epoch 016/020] step 100/843 loss 1.4891\n",
      "[epoch 016/020] step 150/843 loss 1.5276\n",
      "[epoch 016/020] step 200/843 loss 1.4961\n",
      "[epoch 016/020] step 250/843 loss 1.4715\n",
      "[epoch 016/020] step 300/843 loss 1.5386\n",
      "[epoch 016/020] step 350/843 loss 1.6483\n",
      "[epoch 016/020] step 400/843 loss 1.4302\n",
      "[epoch 016/020] step 450/843 loss 1.6594\n",
      "[epoch 016/020] step 500/843 loss 1.7176\n",
      "[epoch 016/020] step 550/843 loss 1.4584\n",
      "[epoch 016/020] step 600/843 loss 1.4442\n",
      "[epoch 016/020] step 650/843 loss 1.5122\n",
      "[epoch 016/020] step 700/843 loss 1.4225\n",
      "[epoch 016/020] step 750/843 loss 1.4896\n",
      "[epoch 016/020] step 800/843 loss 1.6053\n",
      "[epoch 016/020] step 843/843 loss 1.3720\n",
      "[epoch 016/020] train=1.5773  val=1.5660  lr=0.000538137\n",
      "[epoch 017/020] step 50/843 loss 1.5345\n",
      "[epoch 017/020] step 100/843 loss 1.6512\n",
      "[epoch 017/020] step 150/843 loss 1.7239\n",
      "[epoch 017/020] step 200/843 loss 1.6138\n",
      "[epoch 017/020] step 250/843 loss 1.7448\n",
      "[epoch 017/020] step 300/843 loss 1.5443\n",
      "[epoch 017/020] step 400/843 loss 1.5836\n",
      "[epoch 017/020] step 450/843 loss 1.6500\n",
      "[epoch 017/020] step 500/843 loss 1.4651\n",
      "[epoch 017/020] step 550/843 loss 1.6339\n",
      "[epoch 017/020] step 600/843 loss 1.4011\n",
      "[epoch 017/020] step 650/843 loss 1.5509\n",
      "[epoch 017/020] step 700/843 loss 1.5259\n",
      "[epoch 017/020] step 750/843 loss 1.4691\n",
      "[epoch 017/020] step 800/843 loss 1.3805\n",
      "[epoch 017/020] step 843/843 loss 1.4714\n",
      "[epoch 017/020] train=1.5628  val=1.5564  lr=0.000308123\n",
      "[epoch 018/020] step 50/843 loss 1.5610\n",
      "[epoch 018/020] step 100/843 loss 1.5452\n",
      "[epoch 018/020] step 150/843 loss 1.6752\n",
      "[epoch 018/020] step 200/843 loss 1.4306\n",
      "[epoch 018/020] step 250/843 loss 1.6065\n",
      "[epoch 018/020] step 300/843 loss 1.3629\n",
      "[epoch 018/020] step 350/843 loss 1.4953\n",
      "[epoch 018/020] step 400/843 loss 1.4725\n",
      "[epoch 018/020] step 450/843 loss 1.4680\n",
      "[epoch 018/020] step 500/843 loss 1.5252\n",
      "[epoch 018/020] step 550/843 loss 1.5711\n",
      "[epoch 018/020] step 600/843 loss 1.5064\n",
      "[epoch 018/020] step 650/843 loss 1.6745\n",
      "[epoch 018/020] step 700/843 loss 1.6042\n",
      "[epoch 018/020] step 750/843 loss 1.4584\n",
      "[epoch 018/020] step 800/843 loss 1.5795\n",
      "[epoch 018/020] step 843/843 loss 1.4232\n",
      "[epoch 018/020] train=1.5563  val=1.5531  lr=0.000139099\n",
      "[epoch 019/020] step 50/843 loss 1.4788\n",
      "[epoch 019/020] step 100/843 loss 1.4882\n",
      "[epoch 019/020] step 150/843 loss 1.4694\n",
      "[epoch 019/020] step 200/843 loss 1.3942\n",
      "[epoch 019/020] step 250/843 loss 1.6373\n",
      "[epoch 019/020] step 300/843 loss 1.5104\n",
      "[epoch 019/020] step 350/843 loss 1.4359\n",
      "[epoch 019/020] step 400/843 loss 1.5201\n",
      "[epoch 019/020] step 450/843 loss 1.5114\n",
      "[epoch 019/020] step 500/843 loss 1.4171\n",
      "[epoch 019/020] step 550/843 loss 1.4697\n",
      "[epoch 019/020] step 600/843 loss 1.3567\n",
      "[epoch 019/020] step 650/843 loss 1.5641\n",
      "[epoch 019/020] step 700/843 loss 1.2790\n",
      "[epoch 019/020] step 750/843 loss 1.5252\n",
      "[epoch 019/020] step 800/843 loss 1.4221\n",
      "[epoch 019/020] step 843/843 loss 1.4804\n",
      "[epoch 019/020] train=1.5490  val=1.5517  lr=3.57666e-05\n",
      "[epoch 020/020] step 50/843 loss 1.5083\n",
      "[epoch 020/020] step 100/843 loss 1.5091\n",
      "[epoch 020/020] step 150/843 loss 1.3678\n",
      "[epoch 020/020] step 200/843 loss 1.5729\n",
      "[epoch 020/020] step 250/843 loss 1.7235\n",
      "[epoch 020/020] step 300/843 loss 1.4991\n",
      "[epoch 020/020] step 350/843 loss 1.4537\n",
      "[epoch 020/020] step 400/843 loss 1.4816\n",
      "[epoch 020/020] step 450/843 loss 1.4990\n",
      "[epoch 020/020] step 500/843 loss 1.4968\n",
      "[epoch 020/020] step 550/843 loss 1.2871\n",
      "[epoch 020/020] step 600/843 loss 1.5865\n",
      "[epoch 020/020] step 650/843 loss 1.3598\n",
      "[epoch 020/020] step 700/843 loss 1.5689\n",
      "[epoch 020/020] step 750/843 loss 1.6161\n",
      "[epoch 020/020] step 800/843 loss 1.4500\n",
      "[epoch 020/020] step 843/843 loss 1.3606\n",
      "[epoch 020/020] train=1.5463  val=1.5455  lr=1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/23 20:54:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conf_a = TrainConfig(\n",
    "    num_epochs=20,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    lr=0.005,\n",
    "    weight_decay=1e-4,\n",
    "    momentum=0.9,\n",
    "    print_every=50,\n",
    "    tracking_uri=TRACKING_URI,\n",
    "    amp=True,\n",
    "    grad_clip=1.0,\n",
    "    ema_decay=0.999,     \n",
    "    warmup_iters=1000,\n",
    "    scheduler=\"cosine\",\n",
    "    min_lr=1e-6,\n",
    "    freeze_bn=True,\n",
    ") #augmenation is always present\n",
    "\n",
    "\n",
    "trainer_a = Trainer(model, conf_a)\n",
    "hist_a = trainer_a.run(a_train, a_val, experiment_name=EXPERIMENT_PRE_A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3cc001-fe6e-4b1f-8578-697846044e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: ../weights/maskrcnn_attfpn_preA_random.pth\n"
     ]
    }
   ],
   "source": [
    "ckpt_a = WEIGHTS_DIR / f\"{MODEL_NAME}_preA_random.pth\"\n",
    "torch.save(model.state_dict(), ckpt_a)\n",
    "print(\"saved:\", ckpt_a) #mlflow saves checkpoints also\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204ef4d",
   "metadata": {},
   "source": [
    "## Stage 2 — fine-tune on DS B (masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0618821a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neoph/dev/Train/Jupyter/KaryoTest/train/trainer_v2.py:117: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=bool(train_conf.amp and self.device.type == \"cuda\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 001/040] step 50/119 loss 2.8353\n",
      "[epoch 001/040] step 100/119 loss 2.4214\n",
      "[epoch 001/040] step 119/119 loss 2.0971\n",
      "[epoch 001/040] train=2.9200  val=3.3090  lr=4.76e-05\n",
      "[epoch 002/040] step 50/119 loss 1.7504\n",
      "[epoch 002/040] step 100/119 loss 1.8357\n",
      "[epoch 002/040] step 119/119 loss 1.9666\n",
      "[epoch 002/040] train=2.0260  val=3.0342  lr=9.52e-05\n",
      "[epoch 003/040] step 50/119 loss 1.7681\n",
      "[epoch 003/040] step 100/119 loss 1.8856\n",
      "[epoch 003/040] step 119/119 loss 1.8172\n",
      "[epoch 003/040] train=1.8662  val=2.7958  lr=0.0001428\n",
      "[epoch 004/040] step 50/119 loss 1.9004\n",
      "[epoch 004/040] step 100/119 loss 1.8543\n",
      "[epoch 004/040] step 119/119 loss 1.9673\n",
      "[epoch 004/040] train=1.7968  val=2.6043  lr=0.0001904\n",
      "[epoch 005/040] step 50/119 loss 1.6957\n",
      "[epoch 005/040] step 100/119 loss 1.7102\n",
      "[epoch 005/040] step 119/119 loss 1.6801\n",
      "[epoch 005/040] train=1.7511  val=2.4434  lr=0.000238\n",
      "[epoch 006/040] step 50/119 loss 1.7732\n",
      "[epoch 006/040] step 100/119 loss 1.5382\n",
      "[epoch 006/040] step 119/119 loss 1.7730\n",
      "[epoch 006/040] train=1.7034  val=2.2837  lr=0.0002856\n",
      "[epoch 007/040] step 50/119 loss 1.9076\n",
      "[epoch 007/040] step 100/119 loss 1.5219\n",
      "[epoch 007/040] step 119/119 loss 1.5904\n",
      "[epoch 007/040] train=1.6743  val=2.1694  lr=0.0003332\n",
      "[epoch 008/040] step 50/119 loss 1.8738\n",
      "[epoch 008/040] step 100/119 loss 1.7089\n",
      "[epoch 008/040] step 119/119 loss 1.4852\n",
      "[epoch 008/040] train=1.6410  val=2.0617  lr=0.0003808\n",
      "[epoch 009/040] step 50/119 loss 1.3275\n",
      "[epoch 009/040] step 100/119 loss 1.5189\n",
      "[epoch 009/040] step 119/119 loss 1.5178\n",
      "[epoch 009/040] train=1.6116  val=1.9726  lr=0.000399649\n",
      "[epoch 010/040] step 50/119 loss 1.6677\n",
      "[epoch 010/040] step 100/119 loss 1.5199\n",
      "[epoch 010/040] step 119/119 loss 1.5716\n",
      "[epoch 010/040] train=1.5837  val=1.9153  lr=0.000397491\n",
      "[epoch 011/040] step 50/119 loss 1.4881\n",
      "[epoch 011/040] step 100/119 loss 1.4787\n",
      "[epoch 011/040] step 119/119 loss 1.3530\n",
      "[epoch 011/040] train=1.5622  val=1.8480  lr=0.000393388\n",
      "[epoch 012/040] step 50/119 loss 1.7294\n",
      "[epoch 012/040] step 100/119 loss 1.3352\n",
      "[epoch 012/040] step 119/119 loss 1.3658\n",
      "[epoch 012/040] train=1.5521  val=1.8023  lr=0.000387379\n",
      "[epoch 013/040] step 50/119 loss 1.5985\n",
      "[epoch 013/040] step 100/119 loss 1.8133\n",
      "[epoch 013/040] step 119/119 loss 1.6246\n",
      "[epoch 013/040] train=1.5384  val=1.7406  lr=0.000379524\n",
      "[epoch 014/040] step 50/119 loss 1.4564\n",
      "[epoch 014/040] step 100/119 loss 1.7715\n",
      "[epoch 014/040] step 119/119 loss 1.6930\n",
      "[epoch 014/040] train=1.5305  val=1.7052  lr=0.000369901\n",
      "[epoch 015/040] step 50/119 loss 1.6779\n",
      "[epoch 015/040] step 100/119 loss 1.6290\n",
      "[epoch 015/040] step 119/119 loss 1.2871\n",
      "[epoch 015/040] train=1.5148  val=1.6763  lr=0.000358605\n",
      "[epoch 016/040] step 50/119 loss 1.6677\n",
      "[epoch 016/040] step 100/119 loss 1.4311\n",
      "[epoch 016/040] step 119/119 loss 1.5656\n",
      "[epoch 016/040] train=1.5104  val=1.6538  lr=0.000345747\n",
      "[epoch 017/040] step 50/119 loss 1.4423\n",
      "[epoch 017/040] step 100/119 loss 1.4207\n",
      "[epoch 017/040] step 119/119 loss 1.4972\n",
      "[epoch 017/040] train=1.4910  val=1.6225  lr=0.000331454\n",
      "[epoch 018/040] step 50/119 loss 1.5754\n",
      "[epoch 018/040] step 100/119 loss 1.7412\n",
      "[epoch 018/040] step 119/119 loss 1.3810\n",
      "[epoch 018/040] train=1.4902  val=1.5868  lr=0.000315867\n",
      "[epoch 019/040] step 50/119 loss 1.6748\n",
      "[epoch 019/040] step 100/119 loss 1.5110\n",
      "[epoch 019/040] step 119/119 loss 1.4577\n",
      "[epoch 019/040] train=1.4852  val=1.5698  lr=0.000299141\n",
      "[epoch 020/040] step 50/119 loss 1.4832\n",
      "[epoch 020/040] step 100/119 loss 1.3491\n",
      "[epoch 020/040] step 119/119 loss 1.4086\n",
      "[epoch 020/040] train=1.4755  val=1.5597  lr=0.000281441\n",
      "[epoch 021/040] step 50/119 loss 1.4337\n",
      "[epoch 021/040] step 100/119 loss 1.2727\n",
      "[epoch 021/040] step 119/119 loss 1.4604\n",
      "[epoch 021/040] train=1.4775  val=1.5380  lr=0.000262941\n",
      "[epoch 022/040] step 50/119 loss 1.5672\n",
      "[epoch 022/040] step 100/119 loss 1.7506\n",
      "[epoch 022/040] step 119/119 loss 1.3263\n",
      "[epoch 022/040] train=1.4680  val=1.5229  lr=0.000243824\n",
      "[epoch 023/040] step 50/119 loss 1.4587\n",
      "[epoch 023/040] step 100/119 loss 1.5354\n",
      "[epoch 023/040] step 119/119 loss 1.3763\n",
      "[epoch 023/040] train=1.4595  val=1.5246  lr=0.00022428\n",
      "[epoch 024/040] step 50/119 loss 1.5582\n",
      "[epoch 024/040] step 100/119 loss 1.4695\n",
      "[epoch 024/040] step 119/119 loss 1.4592\n",
      "[epoch 024/040] train=1.4590  val=1.5040  lr=0.0002045\n",
      "[epoch 025/040] step 50/119 loss 1.3577\n",
      "[epoch 025/040] step 100/119 loss 1.6106\n",
      "[epoch 025/040] step 119/119 loss 1.3742\n",
      "[epoch 025/040] train=1.4546  val=1.4895  lr=0.000184681\n",
      "[epoch 026/040] step 50/119 loss 1.6113\n",
      "[epoch 026/040] step 100/119 loss 1.5247\n",
      "[epoch 026/040] step 119/119 loss 1.2986\n",
      "[epoch 026/040] train=1.4486  val=1.4865  lr=0.000165018\n",
      "[epoch 027/040] step 50/119 loss 1.3741\n",
      "[epoch 027/040] step 100/119 loss 1.3436\n",
      "[epoch 027/040] step 119/119 loss 1.5755\n",
      "[epoch 027/040] train=1.4483  val=1.4745  lr=0.000145706\n",
      "[epoch 028/040] step 50/119 loss 1.5482\n",
      "[epoch 028/040] step 100/119 loss 1.3524\n",
      "[epoch 028/040] step 119/119 loss 1.2493\n",
      "[epoch 028/040] train=1.4424  val=1.4646  lr=0.000126935\n",
      "[epoch 029/040] step 50/119 loss 1.4566\n",
      "[epoch 029/040] step 100/119 loss 1.4493\n",
      "[epoch 029/040] step 119/119 loss 1.4400\n",
      "[epoch 029/040] train=1.4378  val=1.4710  lr=0.000108891\n",
      "[epoch 030/040] step 50/119 loss 1.4549\n",
      "[epoch 030/040] step 100/119 loss 1.7055\n",
      "[epoch 030/040] step 119/119 loss 1.5462\n",
      "[epoch 030/040] train=1.4354  val=1.4554  lr=9.17513e-05\n",
      "[epoch 031/040] step 50/119 loss 1.7079\n",
      "[epoch 031/040] step 100/119 loss 1.3267\n",
      "[epoch 031/040] step 119/119 loss 1.3932\n",
      "[epoch 031/040] train=1.4333  val=1.4652  lr=7.56861e-05\n",
      "[epoch 032/040] step 50/119 loss 1.3735\n",
      "[epoch 032/040] step 100/119 loss 1.7086\n",
      "[epoch 032/040] step 119/119 loss 1.4571\n",
      "[epoch 032/040] train=1.4360  val=1.4563  lr=6.08537e-05\n",
      "[epoch 033/040] step 50/119 loss 1.4477\n",
      "[epoch 033/040] step 100/119 loss 1.2732\n",
      "[epoch 033/040] step 119/119 loss 1.3516\n",
      "[epoch 033/040] train=1.4381  val=1.4467  lr=4.74007e-05\n",
      "[epoch 034/040] step 50/119 loss 1.5427\n",
      "[epoch 034/040] step 100/119 loss 1.5556\n",
      "[epoch 034/040] step 119/119 loss 1.3211\n",
      "[epoch 034/040] train=1.4366  val=1.4435  lr=3.546e-05\n",
      "[epoch 035/040] step 50/119 loss 1.4461\n",
      "[epoch 035/040] step 100/119 loss 1.5091\n",
      "[epoch 035/040] step 119/119 loss 1.4187\n",
      "[epoch 035/040] train=1.4368  val=1.4433  lr=2.51495e-05\n",
      "[epoch 036/040] step 50/119 loss 1.4108\n",
      "[epoch 036/040] step 100/119 loss 1.3304\n",
      "[epoch 036/040] step 119/119 loss 1.3661\n",
      "[epoch 036/040] train=1.4268  val=1.4327  lr=1.65711e-05\n",
      "[epoch 037/040] step 50/119 loss 1.6602\n",
      "[epoch 037/040] step 100/119 loss 1.6120\n",
      "[epoch 037/040] step 119/119 loss 1.7632\n",
      "[epoch 037/040] train=1.4363  val=1.4278  lr=9.80949e-06\n",
      "[epoch 038/040] step 50/119 loss 1.3285\n",
      "[epoch 038/040] step 100/119 loss 1.5367\n",
      "[epoch 038/040] step 119/119 loss 1.4870\n",
      "[epoch 038/040] train=1.4389  val=1.4280  lr=4.93151e-06\n",
      "[epoch 039/040] step 50/119 loss 1.2950\n",
      "[epoch 039/040] step 100/119 loss 1.4804\n",
      "[epoch 039/040] step 119/119 loss 1.4379\n",
      "[epoch 039/040] train=1.4355  val=1.4251  lr=1.98531e-06\n",
      "[epoch 040/040] step 50/119 loss 1.3089\n",
      "[epoch 040/040] step 100/119 loss 1.5679\n",
      "[epoch 040/040] step 119/119 loss 1.2976\n",
      "[epoch 040/040] train=1.4369  val=1.4329  lr=1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/23 21:52:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: ../weights/maskrcnn_attfpn_A2B_random.pth\n"
     ]
    }
   ],
   "source": [
    "# reset LR schedule I'm just training everything with the identical setup\n",
    "conf_b = TrainConfig(\n",
    "    num_epochs=40,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    lr=0.0004,\n",
    "    weight_decay=1e-4,\n",
    "    momentum=0.9,\n",
    "    print_every=50,\n",
    "    tracking_uri=TRACKING_URI,\n",
    "    amp=True,\n",
    "    grad_clip=1.0,\n",
    "    ema_decay=0.999,\n",
    "    warmup_iters=1000,\n",
    "    scheduler=\"cosine\",\n",
    "    min_lr=1e-6,\n",
    "    freeze_bn=True,\n",
    ")\n",
    "\n",
    "trainer_b = Trainer(model, conf_b)\n",
    "hist_b = trainer_b.run(b_train, b_val, experiment_name=EXPERIMENT_FT_B)\n",
    "\n",
    "ckpt_ab = WEIGHTS_DIR / f\"{MODEL_NAME}_A2B_random.pth\"\n",
    "torch.save(model.state_dict(), ckpt_ab)\n",
    "print(\"saved:\", ckpt_ab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6abc8",
   "metadata": {},
   "source": [
    "## Quick sanity eval (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "593f00f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A test mAP@50: 0.060342345386743546\n",
      "B test metrics keys: ['mAP50', 'PQ_all', 'mPQ', 'PQ_per_class', 'AJI']\n"
     ]
    }
   ],
   "source": [
    "ev = Evaluator(device=DEVICE)\n",
    "\n",
    "a_test = dm.make_loader_a_test()\n",
    "b_test = dm.make_loader_b_test()\n",
    "\n",
    "map50_a = ev.map50(model, a_test)\n",
    "masks_b = ev.metrics_masks(model, b_test, num_classes=NUM_CLASSES)\n",
    "\n",
    "print(\"A test mAP@50:\", map50_a)\n",
    "print(\"B test metrics keys:\", list(masks_b.keys())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57752c7c-d627-4f78-b2fa-092f55f72db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP50': 0.2434900552034378, 'PQ_all': 0.2821934302688574, 'mPQ': 0.2664130457957197, 'PQ_per_class': array([0.35175117, 0.37283354, 0.27804926, 0.28696305, 0.25987666,\n",
      "       0.28920128, 0.29996131, 0.26858043, 0.18557743, 0.19228072,\n",
      "       0.32752773, 0.20500502, 0.33150099, 0.27721892, 0.24712891,\n",
      "       0.29926734, 0.3093991 , 0.28093804, 0.25772101, 0.30437598,\n",
      "       0.35673964, 0.26613559, 0.05417384, 0.09170611,        nan]), 'AJI': 0.3199680921519608}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(masks_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acdce00e-eef1-4441-ad82-49f430ff5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import mlflow.pytorch\n",
    "model_uri = \"file:///media/sdb1/mlflow/195229938318171777/0bf1a57b3e0748fe805a5747839be81f/artifacts/model\"\n",
    "m2 = mlflow.pytorch.load_model(model_uri).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efad93bb-7618-431d-85e9-d1453867bdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A test mAP@50: 0.7488314509391785\n",
      "{'mAP50': 0.10590457171201706, 'PQ_all': 0.02988920014479454, 'mPQ': 0.025426631659907026, 'PQ_per_class': array([0.00650751, 0.        , 0.00564667, 0.01814717, 0.0168132 ,\n",
      "       0.01738305, 0.0064599 , 0.00307618, 0.02126989, 0.0130077 ,\n",
      "       0.03837836, 0.00755604, 0.00957492, 0.01030654, 0.02581946,\n",
      "       0.01400129, 0.03382844, 0.02724894, 0.09051408, 0.03974869,\n",
      "       0.08590034, 0.07388909, 0.00604735, 0.03911434,        nan]), 'AJI': 0.16528589918892436}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "map50_a = ev.map50(m2, a_test)\n",
    "masks_b = ev.metrics_masks(m2, b_test, num_classes=NUM_CLASSES)\n",
    "\n",
    "print(\"A test mAP@50:\", map50_a)\n",
    "print(masks_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed805c23-83f7-46f0-a551-b7eb019244c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== m1 ==\n",
      "custom (class-agnostic): {'mAP50': 0.5262898206710815, 'PQ_all': 0.03672602001273298, 'mPQ': 0.03672602001273298, 'PQ_per_class': array([0.03672602,        nan]), 'AJI': 0.11215135616084955}\n",
      "A test (boxes only):     {'mAP50': 0.18084953725337982}\n",
      "B test (full):           {'mAP50': 0.7555909752845764, 'PQ_all': 0.210108580101161, 'mPQ': 0.211053354715712, 'PQ_per_class': array([0.07364656, 0.08869973, 0.09680295, 0.11778711, 0.12786023,\n",
      "       0.10462205, 0.17621212, 0.16929189, 0.17586031, 0.1862569 ,\n",
      "       0.20593897, 0.20849398, 0.27137837, 0.22846105, 0.26479355,\n",
      "       0.32364428, 0.3005126 , 0.32247135, 0.32722444, 0.28884022,\n",
      "       0.33605031, 0.28595396, 0.16713227, 0.21734532,        nan]), 'AJI': 0.2086153127503059}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets.base import collate_bb\n",
    "from datasets import cfg\n",
    "from train.metrics import get_metrics, compute_map50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATA_CONF = DataConfig(batch_size=1, num_workers=2)\n",
    "DATA = DataModule(DATA_CONF, num_channels=3, with_masks=True)\n",
    "\n",
    "ds_custom = DATA.ds_custom\n",
    "ds_a_test = DATA.ds_a_test\n",
    "ds_b_test = DATA.ds_b_test\n",
    "\n",
    "def make_loader(ds, nw=2):\n",
    "    return DataLoader(ds, batch_size=1, shuffle=False, num_workers=nw, collate_fn=collate_bb)\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect(model, loader, need_masks: bool):\n",
    "    model.eval()\n",
    "    preds, targs = [], []\n",
    "    for images, targets in loader:\n",
    "        images = [im.to(device) for im in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        for out, gt in zip(outputs, targets):\n",
    "            pred = {\n",
    "                \"boxes\": out[\"boxes\"].detach().cpu(),\n",
    "                \"scores\": out[\"scores\"].detach().cpu(),\n",
    "                \"labels\": out[\"labels\"].detach().cpu(),\n",
    "            }\n",
    "            if need_masks:\n",
    "                pred[\"masks\"] = out[\"masks\"][:, 0].detach().cpu().numpy()  # (N,H,W) numpy\n",
    "\n",
    "            targ = {\"boxes\": gt[\"boxes\"].cpu()}\n",
    "            if \"labels\" in gt and torch.is_tensor(gt[\"labels\"]):\n",
    "                targ[\"labels\"] = gt[\"labels\"].cpu()\n",
    "            if need_masks and \"masks\" in gt and torch.is_tensor(gt[\"masks\"]):\n",
    "                targ[\"masks\"] = gt[\"masks\"].cpu().numpy()  # numpy\n",
    "\n",
    "            preds.append(pred)\n",
    "            targs.append(targ)\n",
    "\n",
    "    return preds, targs\n",
    "\n",
    "def eval_custom_class_agnostic(model):\n",
    "    preds, targs = collect(model, make_loader(ds_custom), need_masks=True)\n",
    "    for p in preds:\n",
    "        p[\"labels\"] = torch.ones((len(p[\"boxes\"]),), dtype=torch.int64)\n",
    "    for t in targs:\n",
    "        t[\"labels\"] = torch.ones((len(t[\"boxes\"]),), dtype=torch.int64)\n",
    "    return get_metrics(preds, targs, num_classes=2)\n",
    "\n",
    "def eval_a_boxes(model):\n",
    "    preds, targs = collect(model, make_loader(ds_a_test), need_masks=False)\n",
    "    return {\"mAP50\": compute_map50(preds, targs)}\n",
    "\n",
    "def eval_b_full(model):\n",
    "    preds, targs = collect(model, make_loader(ds_b_test), need_masks=True)\n",
    "    return get_metrics(preds, targs, num_classes=cfg.num_classes)\n",
    "\n",
    "def run(model, name):\n",
    "    model = model.to(device)\n",
    "    print(f\"\\n== {name} ==\")\n",
    "    print(\"custom (class-agnostic):\", eval_custom_class_agnostic(model))\n",
    "    print(\"A test (boxes only):    \", eval_a_boxes(model))\n",
    "    print(\"B test (full):          \", eval_b_full(model))\n",
    "\n",
    "run(model, \"m1\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70dcc3-a83e-4eda-86ec-ffc30924b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== m0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3971002/3894698633.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  m1.load_state_dict(torch.load(\"../weights/maskrcnn_B_ep40.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom (class-agnostic): {'mAP50': 0.3757989704608917, 'PQ_all': 0.025895231791482922, 'mPQ': 0.025895231791482922, 'PQ_per_class': array([0.02589523,        nan]), 'AJI': 0.10967215214188494}\n"
     ]
    }
   ],
   "source": [
    "m1 = build_model(\"maskrcnn_r50_fpn\", int(cfg.num_classes)).to(device)\n",
    "m1.load_state_dict(torch.load(\"../weights/maskrcnn_B_ep40.pth\", map_location=device))\n",
    "run(m1, \"m0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca247d2-3b92-405f-8a67-32310f165c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
