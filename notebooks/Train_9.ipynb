{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b71cdd8a-fc30-46d2-803c-b7ee6eb49cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b298fb1b-3194-4ffb-a389-5b4a6339e3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/neoph/dev/Train/.tenv312/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from models.models import build_model     \n",
    "from datasets.loader import DataModule, DataConfig  \n",
    "from train.trainer_v2 import Trainer, TrainConfig  \n",
    "from train.eval import Evaluator             \n",
    "from datasets.base import collate_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991aa36b-ed88-41b8-885c-808b77f4962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"file:///media/sdb1/mlflow\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 1 + 24\n",
    "model_uri = \"file:///media/sdb1/mlflow/753485487056022103/a7b0ebbbad47442c841112c6bfb35e16/artifacts/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "def allow_missing_masks(model):\n",
    "    orig_forward = model.forward\n",
    "    def forward(images, targets=None):\n",
    "        if model.training and targets is not None and not all((\"masks\" in t) for t in targets):\n",
    "            rh = model.roi_heads\n",
    "            saved = (rh.mask_roi_pool, rh.mask_head, rh.mask_predictor)\n",
    "            try:\n",
    "                rh.mask_roi_pool, rh.mask_head, rh.mask_predictor = None, None, None\n",
    "                return orig_forward(images, targets)\n",
    "            finally:\n",
    "                rh.mask_roi_pool, rh.mask_head, rh.mask_predictor = saved\n",
    "        return orig_forward(images, targets)\n",
    "    model.forward = forward\n",
    "    return model\n",
    "model = allow_missing_masks(model)\n",
    "\n",
    "conf = TrainConfig.from_json(\"./train_9.json\")\n",
    "trainer = Trainer(model, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5203de-16bc-4bd5-8917-4f69b533ae68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def infinite(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "\n",
    "class AlternatingLoader:\n",
    "    def __init__(self, loader_a, loader_b, steps=None, start=\"a\"):\n",
    "        self.a = loader_a\n",
    "        self.b = loader_b\n",
    "        self.steps = steps if steps is not None else 2 * max(len(loader_a), len(loader_b))\n",
    "        self.start = start\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        ia, ib = infinite(self.a), infinite(self.b)\n",
    "        for i in range(self.steps):\n",
    "            if (i % 2 == 0) == (self.start == \"a\"):\n",
    "                yield next(ia)\n",
    "            else:\n",
    "                yield next(ib)\n",
    "\n",
    "dm = DataModule(DataConfig(val_frac=0.1, batch_size=conf.batch_size, num_workers=conf.num_workers), with_masks = False)\n",
    "b_train_loader, b_val_loader = dm.make_loaders_b()\n",
    "\n",
    "a_train_loader = DataLoader(dm.ds_a_train, batch_size=conf.batch_size, shuffle=True, num_workers=conf.num_workers, collate_fn=collate_bb)\n",
    "a_val_loader   = DataLoader(dm.ds_a_val,   batch_size=conf.batch_size, shuffle=False, num_workers=conf.num_workers, collate_fn=collate_bb)\n",
    "\n",
    "mix_train = AlternatingLoader(a_train_loader, b_train_loader)\n",
    "mix_val   = AlternatingLoader(a_val_loader,   b_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f974f7-7f87-476b-aebb-9673010e8a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/19 06:01:58 INFO mlflow.tracking.fluent: Experiment with name 'Att_FT2_Train9' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 001/040] step 50/2250 loss 0.9260\n",
      "[epoch 001/040] step 100/2250 loss 0.5300\n",
      "[epoch 001/040] step 150/2250 loss 0.6465\n",
      "[epoch 001/040] step 200/2250 loss 0.4262\n",
      "[epoch 001/040] step 250/2250 loss 0.3665\n",
      "[epoch 001/040] step 300/2250 loss 0.3969\n",
      "[epoch 001/040] step 350/2250 loss 0.7200\n",
      "[epoch 001/040] step 400/2250 loss 0.6539\n",
      "[epoch 001/040] step 450/2250 loss 0.4258\n",
      "[epoch 001/040] step 500/2250 loss 0.4117\n",
      "[epoch 001/040] step 550/2250 loss 0.5626\n",
      "[epoch 001/040] step 600/2250 loss 0.3577\n",
      "[epoch 001/040] step 650/2250 loss 0.4470\n",
      "[epoch 001/040] step 700/2250 loss 0.3162\n",
      "[epoch 001/040] step 750/2250 loss 0.5005\n",
      "[epoch 001/040] step 800/2250 loss 0.4036\n",
      "[epoch 001/040] step 850/2250 loss 0.4890\n",
      "[epoch 001/040] step 900/2250 loss 0.6818\n",
      "[epoch 001/040] step 950/2250 loss 0.4634\n",
      "[epoch 001/040] step 1000/2250 loss 0.4617\n",
      "[epoch 001/040] step 1050/2250 loss 0.4653\n",
      "[epoch 001/040] step 1100/2250 loss 0.4600\n",
      "[epoch 001/040] step 1150/2250 loss 0.5563\n",
      "[epoch 001/040] step 1200/2250 loss 0.4922\n",
      "[epoch 001/040] step 1250/2250 loss 0.4620\n",
      "[epoch 001/040] step 1300/2250 loss 0.5688\n",
      "[epoch 001/040] step 1350/2250 loss 0.3257\n",
      "[epoch 001/040] step 1400/2250 loss 0.4611\n",
      "[epoch 001/040] step 1450/2250 loss 0.3932\n",
      "[epoch 001/040] step 1500/2250 loss 0.4613\n",
      "[epoch 001/040] step 1550/2250 loss 0.3705\n",
      "[epoch 001/040] step 1600/2250 loss 0.3916\n",
      "[epoch 001/040] step 1650/2250 loss 0.4068\n",
      "[epoch 001/040] step 1700/2250 loss 0.3627\n",
      "[epoch 001/040] step 1750/2250 loss 0.4544\n",
      "[epoch 001/040] step 1800/2250 loss 0.3977\n",
      "[epoch 001/040] step 1850/2250 loss 0.4966\n",
      "[epoch 001/040] step 1900/2250 loss 0.4226\n",
      "[epoch 001/040] step 1950/2250 loss 0.4433\n",
      "[epoch 001/040] step 2000/2250 loss 0.3820\n",
      "[epoch 001/040] step 2050/2250 loss 0.3595\n",
      "[epoch 001/040] step 2100/2250 loss 0.3494\n",
      "[epoch 001/040] step 2150/2250 loss 0.6073\n",
      "[epoch 001/040] step 2200/2250 loss 0.3949\n",
      "[epoch 001/040] step 2250/2250 loss 0.3370\n",
      "[epoch 001/040] train=0.4707  val=0.4758  lr=0.00499757\n",
      "[epoch 002/040] step 50/2250 loss 0.4888\n",
      "[epoch 002/040] step 100/2250 loss 0.3949\n",
      "[epoch 002/040] step 150/2250 loss 0.3821\n",
      "[epoch 002/040] step 200/2250 loss 0.5103\n",
      "[epoch 002/040] step 250/2250 loss 0.5543\n",
      "[epoch 002/040] step 300/2250 loss 0.3921\n",
      "[epoch 002/040] step 350/2250 loss 0.4370\n",
      "[epoch 002/040] step 400/2250 loss 0.5132\n",
      "[epoch 002/040] step 450/2250 loss 0.3859\n",
      "[epoch 002/040] step 500/2250 loss 0.4852\n",
      "[epoch 002/040] step 550/2250 loss 0.3722\n",
      "[epoch 002/040] step 600/2250 loss 0.4203\n",
      "[epoch 002/040] step 650/2250 loss 0.5315\n",
      "[epoch 002/040] step 700/2250 loss 0.5574\n",
      "[epoch 002/040] step 750/2250 loss 0.3662\n",
      "[epoch 002/040] step 800/2250 loss 0.3740\n",
      "[epoch 002/040] step 850/2250 loss 0.3148\n",
      "[epoch 002/040] step 900/2250 loss 0.4043\n",
      "[epoch 002/040] step 950/2250 loss 0.3342\n",
      "[epoch 002/040] step 1000/2250 loss 0.4553\n",
      "[epoch 002/040] step 1050/2250 loss 0.4859\n",
      "[epoch 002/040] step 1100/2250 loss 0.3493\n",
      "[epoch 002/040] step 1150/2250 loss 0.4010\n",
      "[epoch 002/040] step 1200/2250 loss 0.3816\n",
      "[epoch 002/040] step 1250/2250 loss 0.4153\n",
      "[epoch 002/040] step 1300/2250 loss 0.4400\n",
      "[epoch 002/040] step 1350/2250 loss 0.4737\n",
      "[epoch 002/040] step 1400/2250 loss 0.4251\n",
      "[epoch 002/040] step 1450/2250 loss 0.3461\n",
      "[epoch 002/040] step 1500/2250 loss 0.4136\n",
      "[epoch 002/040] step 1550/2250 loss 0.3361\n",
      "[epoch 002/040] step 1600/2250 loss 0.4356\n",
      "[epoch 002/040] step 1650/2250 loss 0.4387\n",
      "[epoch 002/040] step 1700/2250 loss 0.3720\n",
      "[epoch 002/040] step 1750/2250 loss 0.3671\n",
      "[epoch 002/040] step 1800/2250 loss 0.4260\n",
      "[epoch 002/040] step 1850/2250 loss 0.4104\n",
      "[epoch 002/040] step 1900/2250 loss 0.3842\n",
      "[epoch 002/040] step 1950/2250 loss 0.5521\n",
      "[epoch 002/040] step 2000/2250 loss 0.5156\n",
      "[epoch 002/040] step 2050/2250 loss 0.3935\n",
      "[epoch 002/040] step 2100/2250 loss 0.4167\n",
      "[epoch 002/040] step 2150/2250 loss 0.3363\n",
      "[epoch 002/040] step 2200/2250 loss 0.4856\n",
      "[epoch 002/040] step 2250/2250 loss 0.4018\n",
      "[epoch 002/040] train=0.4457  val=0.4674  lr=0.00498095\n",
      "[epoch 003/040] step 50/2250 loss 0.3579\n",
      "[epoch 003/040] step 100/2250 loss 0.3971\n",
      "[epoch 003/040] step 150/2250 loss 0.4201\n",
      "[epoch 003/040] step 200/2250 loss 0.4320\n",
      "[epoch 003/040] step 250/2250 loss 0.5415\n",
      "[epoch 003/040] step 300/2250 loss 0.3566\n",
      "[epoch 003/040] step 350/2250 loss 0.3579\n",
      "[epoch 003/040] step 400/2250 loss 0.4677\n",
      "[epoch 003/040] step 450/2250 loss 0.3406\n",
      "[epoch 003/040] step 500/2250 loss 0.5617\n",
      "[epoch 003/040] step 550/2250 loss 0.3343\n",
      "[epoch 003/040] step 600/2250 loss 0.3681\n",
      "[epoch 003/040] step 650/2250 loss 0.3516\n",
      "[epoch 003/040] step 700/2250 loss 0.5129\n",
      "[epoch 003/040] step 750/2250 loss 0.4345\n",
      "[epoch 003/040] step 800/2250 loss 0.7143\n",
      "[epoch 003/040] step 850/2250 loss 0.3840\n",
      "[epoch 003/040] step 900/2250 loss 0.5810\n",
      "[epoch 003/040] step 950/2250 loss 0.3684\n",
      "[epoch 003/040] step 1000/2250 loss 0.3998\n",
      "[epoch 003/040] step 1050/2250 loss 0.5444\n",
      "[epoch 003/040] step 1100/2250 loss 0.4081\n",
      "[epoch 003/040] step 1150/2250 loss 0.4271\n",
      "[epoch 003/040] step 1200/2250 loss 0.4507\n",
      "[epoch 003/040] step 1250/2250 loss 0.4348\n",
      "[epoch 003/040] step 1300/2250 loss 0.4446\n"
     ]
    }
   ],
   "source": [
    "hist_ft = trainer.run(mix_train, mix_val, experiment_name=\"Att_FT2_Train9\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c98216c0-e163-42d0-b008-f8c66ada5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model.state_dict(), \"../weights/maskrcnn_attfpn30.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276566d-26a3-4892-a51d-da0c794bbea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
